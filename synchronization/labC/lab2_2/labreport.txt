2.2. Синхронизация доступа к разделяемому ресурсу
    a. Измените реализацию очереди, добавив спинлок для синхронизации доступа к
        разделяемым данным.
    b. Убедитесь, что не возникает ошибок передачи данных через очередь.
    c. Поиграйте параметрами из пункта 2.1:
        i. Оцените загрузку процессора.
        ii. Оцените время проведенное в пользовательском режиме и в режиме
            ядра.
        iii. Оцените текущую заполненность очереди, количество попыток
            чтения-записи и количество прочитанных-записанных данных.
        iv. Объясните наблюдаемые результаты.
    d. Часто бывает, что поток, пишущий данные в очередь вынужден ожидать их
        (например, из сети на select()/poll()). Проэмулируйте эту ситуацию, добавив в
        поточную функцию писателя периодический вызов usleep(1). Выполните
        задания из пункта с.
    e. Измените реализацию очереди, заменив спинлок на мутекс. Проделайте
        задания из пунктов b, c и d. Сравните со спинлоком.
    f. Измените реализацию очереди, добавив условную переменную. Проделайте
        задания из пунктов b, c и d. Сравните со спинлоком и мутексом.
    g. Используйте для синхронизации доступа к очереди семафоры. Проделайте
        задания из пунктов b, c и d. Сравните со спинлоком, мутексом и условной
        переменной.



a. Измените реализацию очереди, добавив спинлок для синхронизации доступа к
    разделяемым данным.
b. Убедитесь, что не возникает ошибок передачи данных через очередь.

    в теримнале выполняю: 
        gcc queue-threads.c queue.c -o sync && ./sync
    вывод:
        бесконечный, нормальный при любом макисмальном количестве узлов процесс

    что добавил: 
        queue.h
            typedef struct {
                atomic_int lock;
            } spinlock_t;

            typedef struct _Queue {
                ...
                spinlock_t lock; 
                ...
            }

        queue.c
            static void spinlock_init(spinlock_t *s) 
                инициализирует поле lock структуры спинлока в разблокированное состояние. Я использую atomic_init lock 
                    в качестве флага, где 
                        1: лок свободен, 
                        0: лок занят. 
                    После вызова spinlock_init(), 
                    любой поток, вызвавший spinlock_lock(), увидит что mylock.lock == 1 и сможет сменить его на 0, 
                    захватив лок:
                    atomic_init(&s->lock, 1);

            static void spinlock_lock(spinlock_t *s)
                пытаемся захватить лок, пока другим потоком он не будет разблокирован. На каждой итерации сравниваем текущее значение
                    s->lock с 1: 
                        если они равны, значит лок свободен, теперь s->lock = 0 (заблокировал). Захват лока произведен успешно -> break;
                        если нет, то какой-то другой поток уже захватил лок. Повторяем попытку в новой итерации:
                while (1) {
                    int one = 1;
                    if (atomic_compare_exchange_strong(&s->lock, &one, 0)) {
                        break;
                    }
                }

            static void spinlock_unlock(spinlock_t *s)
                освобождение лока, переводя его флаг в разблокированное состояние: 
                int zero = 0;
                atomic_compare_exchange_strong(&s->lock, &zero, 1);

        ---------------------------------------------------------------------------------------------------------------------------------------
        atomic_int
            тип данных, который, в отличие от простого int, гарантирует атомарность всех операций с ним.

        ? почему используем atomic_int а не int?
        - чтобы не было гонок между потоками при проверке и установке флага lock. Если бы взяли int, то например два потока 
            могли бы прочитать лок как 1 и оба записать 0, посчитав, что они захватили лок. Но 2 потока не могут захватывать лок одновременно.
        
        atomic_init
            void atomic_init(volatile atomic_int *object, atomic_int desired)
                object - указатель на переменную типа atomic_int, которую нужно инициализировать.
                desired - значение, с которым должна быть инициализирована эта переменная.

            используется для инициализации атомарного объекта. Устанавливает начальное значение атомарной переменной.

            в моем случае: 
                atomic_init(&s->lock, 1);

        atomic_compare_exchange_strong
            bool atomic_compare_exchange_strong(volatile atomic_int *object, atomic_int *expected, atomic_int desired)
                object - указатель на переменную типа atomic_int, которую нужно изменить.
                expected - указатель на переменную типа atomic_int, содержащую ожидаемое значение.
                desired - новое значение, которое нужно установить.

            сравнивает текущее значение атомарной переменной object со значением, указанным в expected. Если они равны, то устанавливает 
                значение desired в object и возвращает trut. В противном случае, оставляет object без изменений и возвращает false.

            в моем случае:
                if (atomic_compare_exchange_strong(&s->lock, &one, 0)) {
                    break;
                }
            
            https://spec-zone.ru/cpp/atomic/atomic_compare_exchange



c. Поиграйте параметрами из пункта 2.1:
    i. Оцените загрузку процессора.
    ii. Оцените время проведенное в пользовательском режиме и в режиме
        ядра.
    iii. Оцените текущую заполненность очереди, количество попыток
        чтения-записи и количество прочитанных-записанных данных.
    iv. Объясните наблюдаемые результаты.

    i. для оценки загрузки процессора используем htop - в отдельном терминале запускаю и показываю следующее:
        работу потоков процесса sync на одном ядре (у reader() и writer() set_cpu(1);) - в htop вижу что загрузка ядра 1 100%
        работу потоков процесса sync на двух ядрах (у reader() и writer() set_cpu(1 и 2) соответственно) - загрузка ядер 1 и 2 100%

    ii. для оценки времени компилирую и запускаю с time:
            gcc queue-threads.c queue.c -o sync && time ./sync
            
            на одном ядре вижу примерно одинаковое real и user время работы: 
                real    0m6,457s
                user    0m6,439s
                sys     0m0,003s
            в то время как на двух ядрах user время аккурат в два раза больше real:
                real    0m6,445s
                user    0m12,887s
                sys     0m0,001s
            user - сумма времени, проведённого в user mode всеми потоками, коих активных у меня 2: reader() и writer(), 
                при этом оба потребляют CPU на 100%, в чем мы убедились в предыдущем пункте.
            sys в обоих случаях примерно нулевой так как не делаю операций в ядре (раз в секунду sleep(1) у монитора, а 
                всё остальное - чисто пользовательские операции).

    iii. в терминале пишу:
            gcc queue-threads.c queue.c -o sync && ./sync

        при работе на одном ядре:
            queue stats: current size 100000; attempts: (17904911 17280112 624799); counts (4009080 3909080 100000)
            queue stats: current size 100000; attempts: (34557319 35505118 -947799); counts (7592519 7492519 100000)
            queue stats: current size 0; attempts: (52666334 52964686 -298352); counts (11349572 11349572 0)
            queue stats: current size 0; attempts: (68224857 71711105 -3486248); counts (15281929 15281929 0)
            queue stats: current size 0; attempts: (85929200 89077621 -3148421); counts (18741021 18741021 0)
            queue stats: current size 100000; attempts: (99598339 110323665 -10725326); counts (22002535 21902535 100000)
        при работе потоков на двух ядрах:
            queue stats: current size 0; attempts: (356815 10721205 -10364390); counts (356815 356815 0)
            queue stats: current size 0; attempts: (822012 21194370 -20372358); counts (822012 822012 0)
            queue stats: current size 0; attempts: (1220246 31843701 -30623455); counts (1220246 1220246 0)
            queue stats: current size 0; attempts: (1710253 42277508 -40567255); counts (1710253 1710253 0)
            queue stats: current size 0; attempts: (2084787 52976020 -50891233); counts (2084787 2084787 0)
            queue stats: current size 0; attempts: (2571947 63414633 -60842686); counts (2571947 2571947 0)
        
        При работе потоков на двух ядрах вижу что add_count +- = get_count, а при работе потоков на одном ядре они не равны.
            на одном ядре writer() работает быстрее reader(), поэтому текущий размер очереди count быстро подходит к max_count, 
            поэтому writer() имеет большое количество add_attempts, но они не превращаются в реальные новые элементы в очереди.
            Значит writer() чаще освобождает лок спинлока нежели reader(). Поалагаю часто срабатывает это условие: 
                if (q->count == q->max_count) {
                    spinlock_unlock(&q->lock);
                    return 0;
                }
        При работе на двух ядрах оба потока выполняются параллельно, значит writer() не имеет вышеописанную проблему.
            reader() и writer() примерно одинаково быстро захватывают и освобождают лок, что и приводит к add_count +- = get_count.

    

d. Часто бывает, что поток, пишущий данные в очередь вынужден ожидать их
    (например, из сети на select()/poll()). Проэмулируйте эту ситуацию, добавив в
    поточную функцию писателя периодический вызов usleep(1). Выполните
    задания из пункта с.

    usleep
        int usleep(useconds_t usec); 
            usec - количество микросекунд на которое нужно приостановить выполнение процесса
        
        функция приостанавливает выполнение процесса на usec микросекунд. 
        ! Остановка может продлиться несколько больше из-за системной активности или из-за того, что для осуществления вызова 
            требуется определенное время.  

    i. работа потоков процесса sync на одном ядре (у reader() и writer() set_cpu(1)) - в htop вижу что загрузка ядра 1 100%
        работу потоков процесса sync на двух ядрах (у reader() и writer() set_cpu(1 и 2) соответственно) - загрузка ТОЛЬКО ядра 1 100%

        Из-за usleep(1) в бесконечном цикле действие потока writer() делает очень мало попыток добавления элемента в очередь - что видно в выводе:
            queue stats: current size 0; attempts: (3768 21806832 -21803064); counts (3768 3768 0)
            queue stats: current size 0; attempts: (7898 44795723 -44787825); counts (7898 7898 0)
            queue stats: current size 0; attempts: (12900 70565760 -70552860); counts (12900 12900 0)
            queue stats: current size 0; attempts: (16045 90844745 -90828700); counts (16045 16045 0)

    ii. в терминале пишу: 
            gcc queue-threads.c queue.c -o sync && time ./sync
        
        при работе на одном ядре: 
            real    0m4,523s
            user    0m4,467s
            sys     0m0,004s
        при работе на двух ядрах:
            real    0m4,434s
            user    0m4,583s
            sys     0m0,005s

        теперь user всегда незначитально превышает real, но не в 2 раза, как это было без usleep(1) на writer() потоке.
            В каждой итерации writer() спит и его ядро освобождается, и даже при двух зайдействованных ядрах, практически всегда 
            работает максимум один активный поток - reader().
            При этом до usleep(1) - когда writer() выполняет:
                int ok = queue_add(q, i);
            благодаря queue_add() у user время работы всегда немного больше чем у real + из определения usleep() следует 
            что оно может занимать несколько больше времени, так как для осуществления вызова требуется определенное время. 
    
    iii. Из i.: из-за usleep(1) writer() делает очень мало операций вставки элементов, по сравнению с reader() - что видно во 
        всех выводах выше. Поэтому количество реальных операций по успешному добавлению/извлечению элементов из очереди 
        всегла совпадает - reader() как только видит появившийся элемент в очереди, сразу его удаляет, а writer() не успевает восполнять. 



e. Измените реализацию очереди, заменив спинлок на мутекс. Проделайте
    задания из пунктов b, c и d. Сравните со спинлоком.

    ? mutex vs spinlock
    -   mutex 
            когда поток пытается захватить заблокированный мьютекс:
                он блокируется и переводится в состояние ожидания (sleep)
                управление передается ядру ОС -> не тратит CPU
            когда мьютекс освобождается (какой-то второй поток который держал мьютекс его разблокирует):
                ОС пробуждает наш ожидающий поток (wake up)
                при получении кванта времени поток продолжит выполнение
            ! использует сискол futex -> требуется переключение контекста. (НО не потребляет CPU пока спит)

        spinlock
            когда поток пытается захватить заблокированный спинлок:
                поток продолжает в цикле проверять состояние лока и пытается заблокировать (значит другой поток не может занять его место)
                не освобождает CPU
            ! работает полностью в user space -> не требует переключения контекста
            ! 100% загрузка ядра CPU из-за цикла ожидания блокировки

        обобщение:
            при заблокированном мьютексе поток переводится в сон; при заблокированном локе поток постоянно пытается заблокировать.
            мьютекс использует сисколы, спинлок - нет. 
            при спящем мьютексе ОС передает ядро активному потоку, при спинлоке долбящийся поток продолжит занимать ядро.

        ответ: https://stackoverflow.com/questions/5869825/when-should-one-use-a-spinlock-instead-of-mutex

    в терминале выполняю:
        cd ../e
        gcc queue-threads.c queue.c -o sync && ./sync
    что изменил:
        queue.h
            typedef struct _Queue {
                ...
                pthread_mutex_t lock;
                ...
            }

        queue.c
            queue_t* queue_init(int max_count) {
                ...
                pthread_mutex_init(&q->lock, NULL);
                ...
            }

            void queue_destroy(queue_t *q) {
                ...
                pthread_mutex_destroy(&q->lock);
                ...
            }

        ---------------------------------------------------------------------------------------------------------------------------------------
        pthread_mutex_init
            int pthread_mutex_init(pthread_mutex_t *restrict mutex, const pthread_mutexattr_t *restrict attr);
                mutex - указатель на объект типа pthread_mutex_t, который будет инициализирован как мьютекс.
                attr - указатель на объект типа pthread_mutexattr_t, содержащий атрибуты мьютекса. 

            инициализирует мьютекс, используя атрибуты, указанные объектом атрибутов мьютекса attr. 
                Если attr равен NULL, то мьютекс инициализируется с атрибутами по умолчанию (см. pthread_mutexattr_init()). 
                После инициализации мьютекс находится в разблокированном состоянии.

            в моем случае: 
                queue_t* queue_init(int max_count) {
                    ...
                    pthread_mutex_init(&q->lock, NULL);
                    ...
                }
            
            https://help.kpda.ru/neutrino/2020/help/index.jsp?topic=%2Fru.kpda.doc.os_ru%2Fhtml%2Flibraries%2Flibc%2Fp%2Fpthread_mutex_init.html
            https://www.opennet.ru/cgi-bin/opennet/man.cgi?topic=pthread_mutex_init
        
        pthread_mutex_destroy
            int pthread_mutex_destroy(pthread_mutex_t *mutex);
                mutex - указатель на объект типа pthread_mutex_t, который будет уничтожен.

            уничтожает незаблокированный мьютекс mutex. Уничтожить заблокированный мьютекс можно только 
                если вы являетесь владельцем этого мьютекса.

            в моем случае:
                void queue_destroy(queue_t *q) {
                    ...
                    pthread_mutex_destroy(&q->lock);
                    ...
                }
            
            https://help.kpda.ru/neutrino/2020/help/index.jsp?topic=%2Fru.kpda.doc.os_ru%2Fhtml%2Flibraries%2Flibc%2Fp%2Fpthread_mutex_init.html
            https://www.opennet.ru/cgi-bin/opennet/man.cgi?topic=pthread_mutex_init
        
        pthread_mutex_lock
            int pthread_mutex_lock( pthread_mutex_t *mutex );
                mutex - указатель на объект типа pthread_mutex_t, который будет заблокирован.

            блокирует мьютекс mutex. Если мьютекс уже заблокирован, то вызывающий поток блокируется до тех пор, 
                пока он не захватит мьютекс. По возвращении из функции, объект мьютекса блокируется и принадлежит 
                вызывающему потоку.

            в моем случае:
                int queue_add(queue_t *q, int val) {
                    pthread_mutex_lock(&q->lock);
                    ...
                }
            
            https://help.kpda.ru/neutrino/2020/help/index.jsp?topic=%2Fru.kpda.doc.os_ru%2Fhtml%2Flibraries%2Flibc%2Fp%2Fpthread_mutex_init.html
            https://www.opennet.ru/man.shtml?topic=pthread_mutex_lock&category=3&russian=1

        pthread_mutex_unlock
            int pthread_mutex_unlock( pthread_mutex_t *mutex );
                mutex - указатель на объект типа pthread_mutex_t, который будет разблокирован.

            в моем случае: 
                int queue_add(queue_t *q, int val) {
                    ...
                    pthread_mutex_unlock(&q->lock);
                }

            разблокирует мьютекс mutex. Мьютекс должен принадлежать вызывающему потоку. Если в мьютексе есть 
                заблокированные потоки, ожидающий поток с наивысшим приоритетом разблокируется и становится 
                следующим владельцем мьютекса.

            https://help.kpda.ru/neutrino/2020/help/index.jsp?topic=%2Fru.kpda.doc.os_ru%2Fhtml%2Flibraries%2Flibc%2Fp%2Fpthread_mutex_init.html
            https://www.opennet.ru/cgi-bin/opennet/man.cgi?topic=pthread_mutex_unlock

    i. Оцените загрузку процессора.
        для оценки загрузки процессора используем htop - в отдельном терминале запускаю и показываю следующее:
            работу потоков процесса sync на одном ядре (у reader() и writer() set_cpu(1);) - в htop вижу что загрузка ядра 1 ПОЧТИ 100%
            работу потоков процесса sync на двух ядрах (у reader() и writer() set_cpu(1 и 2) соответственно) - загрузка ядер 1 и 2 ПОЧТИ 100%

        при mutex-е нагрузка на ядра стабильно незначительно ниже, чем при спинлоке.

    ii. Оцените время проведенное в пользовательском режиме и в режиме ядра.

        в терминале пишу: 
            gcc queue-threads.c queue.c -o sync && time ./sync

        при работе на одном ядре:
            real    0m5,513s
            user    0m5,511s
            sys     0m0,002s             
        при работе на двух ядрах:
            real    0m5,448s
            user    0m4,836s
            sys     0m4,079s

        при работе на одном ядре время совпадает с результатами, полученными при спинлоке, но при работе на двух ядрах результаты отличаются
            от аналогичных замеров у спинлока: 
                real    0m6,445s
                user    0m12,887s
                sys     0m0,001s
            ! вижу что системное время возросло (использование сискола futex(2) + переключения контекста), а пользовательское 
            уменьшилось, так как опять же переключение контекста (теперь мы не пребываем всегда в user space).
            
    iii. Оцените текущую заполненность очереди, количество попыток 
        чтения-записи и количество прочитанных-записанных данных.

        в терминале пишу:
            gcc queue-threads.c queue.c -o sync &&  ./sync

        при работе на одном ядре:
            queue stats: current size 100000; attempts: (20990816 26454828 -5464012); counts (12464902 12364902 100000)
            queue stats: current size 69992; attempts: (42316718 53364817 -11048099); counts (24501477 24431485 69992)
            queue stats: current size 45176; attempts: (62913320 79566687 -16653367); counts (37218011 37172835 45176)
            queue stats: current size 74063; attempts: (84084861 106235237 -22150376); counts (49404671 49330608 74063)
        при работе на двух ядрах:
            queue stats: current size 99978; attempts: (7154244 4570902 2583342); counts (4669411 4569433 99978)
            queue stats: current size 99990; attempts: (15019434 8350706 6668728); counts (8449227 8349237 99990)
            queue stats: current size 99994; attempts: (22789245 11973010 10816235); counts (12071535 11971541 99994)
            queue stats: current size 99959; attempts: (30663112 15952662 14710450); counts (16051152 15951193 99959)
        
        при работе на одном ядре сохраняется схожий спинлоку разброс, а при работе на двух ядрах - нет. 
            В спинлоке при двух ядрах было так: reader() и writer() примерно одинаково быстро захватывают и 
            освобождают лок, что приводило к add_count +- = get_count. Сейчас при захвате мьютекса видим, 
            что add_count и get_count не равны при работе на двух ядрах. 
            ? Почему?
            - когда writer() в queue_add() pthread_mutex_unlock() (futex_wake(2)), у ядра есть список потоков, 
                ожидающих разблокировки мьютекса. ОС-й может быть выбран поток, который ранее освободил мьютекс - это 
                только что сделал writer() - его запись свежее reader()-а. 
                ИТОГО: самоотсос - writer() почти сразу получает мьютекс снова, делает queue_add(), отпускает мьютекс, 
                    забирается из списка ожидающих потоков, ...  

                    

    Часто бывает, что поток, пишущий данные в очередь вынужден ожидать их
        (например, из сети на select()/poll()). Проэмулируйте эту ситуацию, добавив в
        поточную функцию писателя периодический вызов usleep(1). Выполните
        задания из пункта с:

        i. Оцените загрузку процессора.
            АНАЛОГИЧНО СПИНЛОКУ: 
                работа потоков процесса sync на одном ядре (у reader() и writer() set_cpu(1)) - в htop вижу что загрузка ядра 1 100%
                работа потоков процесса sync на двух ядрах (у reader() и writer() set_cpu(1 и 2) соответственно) - загрузка ТОЛЬКО ядра 1 100%

                Из-за usleep(1) в бесконечном цикле действие потока writer() делает очень мало попыток добавления элемента в 
                очередь - что видно в выводе:
                    main [34216 4296 34216]
                    qmonitor: [34216 4296 34217]
                    queue stats: current size 0; attempts: (0 0 0); counts (0 0 0)
                    reader [34216 4296 34218]
                    set_cpu: set cpu 1
                    writer [34216 4296 34219]
                    set_cpu: set cpu 2
                    queue stats: current size 0; attempts: (18654 69198083 -69179429); counts (18654 18654 0)
                    queue stats: current size 0; attempts: (37624 139127000 -139089376); counts (37624 37624 0)
                    queue stats: current size 0; attempts: (56614 209008280 -208951666); counts (56614 56614 0)
                    queue stats: current size 0; attempts: (75530 278887229 -278811699); counts (75530 75530 0)
                    ^C

        ii. Оцените время проведенное в пользовательском режиме и в режиме ядра.

            в терминале пишу: 
                gcc queue-threads.c queue.c -o sync && time ./sync

            при работе на одном ядре:
                real    0m5,432s
                user    0m5,416s
                sys     0m0,004s
            при работе на двух ядрах:
                real    0m5,570s
                user    0m5,781s
                sys     0m0,008s

            было на двух ядрах, без usleep(1) (см. e.c.ii. (мы сейчас в e.d.ii.)): 
                real    0m5,448s
                user    0m4,836s
                sys     0m4,079s
            теперь user всегда незначитально превышает real, но не в 2 раза, как это было без usleep(1) на writer() потоке.
                при двух задействованных ядрах, практически всегда работает чаще всего один активный поток - reader().
                При этом до usleep(1) writer() выполняет это:
                    int ok = queue_add(q, i);
                - благодаря queue_add() у user время работы всегда немного больше чем у real + из определения usleep() следует 
                что оно может занимать несколько больше времени, так как для осуществления вызова требуется определенное время.
            
            ? куда делось sys время работы?
            - sys - время, которое процесс провёл АКТИВНО внутри ядра (на системных вызовах: futex, nanosleep, read, write, ...).
                без usleep(1) потоки "сонно" борются за lock - если один уже захватил мьютекс, то у другого попытка 
                    pthread_mutex_lock() вызывает сискол futex_wait(), а потом futex_wake при разблокировке мьютекса.
                с usleep(1) поток writer() после каждой попытки выполнить добавление элемента в queue_add() уходит в nanosleep() 
                    (сискол вызываемый usleep()), и при этом поток уходит в состояние ожидания (sleep), не потребляя CPU.
                
                ИТОГО: sys почти занулен так как нет futex_wait()/futex_wake(), вместо них у writer()-а идут nanosleep(), 
                    а он не засчитывается в sys в sys идет только то время, которое CPU тратил на ВЫПОЛНЕНИЕ (а writer() спит) 
                    потока в kernel mode.

        iii. Оцените текущую заполненность очереди, количество попыток 
            чтения-записи и количество прочитанных-записанных данных.

            в терминале пишу: 
                gcc queue-threads.c queue.c -o sync && ./sync

            при работе на одном ядре:
                queue stats: current size 0; attempts: (17928 64360387 -64342459); counts (17928 17928 0)
                queue stats: current size 0; attempts: (36233 130787003 -130750770); counts (36233 36233 0)
                queue stats: current size 0; attempts: (54575 197254380 -197199805); counts (54575 54575 0)
                queue stats: current size 0; attempts: (72880 263781876 -263708996); counts (72880 72880 0)
            при работе на двух ядрах:
                queue stats: current size 0; attempts: (18814 68282255 -68263441); counts (18814 18814 0)
                queue stats: current size 0; attempts: (37812 138118505 -138080693); counts (37812 37812 0)
                queue stats: current size 0; attempts: (56808 208014852 -207958044); counts (56808 56808 0)
                queue stats: current size 0; attempts: (75790 277929588 -277853798); counts (75790 75790 0)

            Из i.: из-за usleep(1) writer() делает очень мало операций вставки элементов, по сравнению с reader() - что видно во 
                всех выводах выше. Поэтому количество реальных операций по успешному добавлению/извлечению элементов из очереди 
                всегла совпадает - reader() как только видит появившийся элемент в очереди, сразу его удаляет, а writer() 
                не успевает восполнять. 



f. Измените реализацию очереди, добавив условную переменную. Проделайте
    задания из пунктов b, c и d. Сравните со спинлоком и мутексом.

    ? mutex+cd vs mutex
    -   mutex: гарантия взаимного исключения критических участков кода:
                pthread_mutex_lock(&mutex);
                // критический участок кода - только один поток может работать с этой областью в единицу времени
                pthread_mutex_unlock(&mutex);
            Его проблема в том что можно сделать активное ожидание, ТРАТЯ CPU:
                while (1) {
                    if (pthread_mutex_trylock(&mutex) == 0) {
                        break;
                    }
                    // активное ожидание
                    usleep(1);
                }
            То есть вместо сна поток будет активно работать, пока не получит доступ к мьютексу.

        mutex+cd: ожидание конкретных событий в сочетании с защитой общих данных. Потоки ждут 
            наступления определенных значений у условных переменных. 
            Состоит из 2 элементов:
                1) мьютекс: защита доступа к общим данным
                2) условная переменная: механизм сигнализации о изменении состояния. 
                    Это способ засыпать внутри мьютекса только тогда, когда 
                    её состояние неподходящее, и просыпаться тогда, когда другой поток скажет, что она изменилась. 

        Итак: для ожидания условий использую связку mutex+cd.

    в терминале выполняю:
        cd ../f
        gcc queue-threads.c queue.c -o sync && ./sync
    что изменил:
        queue.h
            typedef struct _Queue {
                ...
                pthread_cond_t not_full;
                pthread_cond_t not_empty;
            }

        queue.c
            queue_t* queue_init(int max_count) {
                ...
                pthread_cond_init (&q->not_full, NULL);
                pthread_cond_init (&q->not_empty, NULL);
                ...
            }

            void queue_destroy(queue_t *q) {
                ...
                pthread_cond_broadcast(&q->not_full);
                pthread_cond_broadcast(&q->not_empty);
                ...
                pthread_cond_destroy(&q->not_full);
                pthread_cond_destroy(&q->not_empty);
                ...
            }

            int queue_add(queue_t *q, int val) {
                ...
                while (q->count == q->max_count) {
                    pthread_cond_wait(&q->not_full, &q->lock);
                }
                ...
                pthread_cond_signal(&q->not_empty);
                ...
            }

            int queue_get(queue_t *q, int *val) {
                ...
                while (q->count == 0) {
                    pthread_cond_wait(&q->not_empty, &q->lock);
                }
                ...
                pthread_cond_signal(&q->not_full);
                ...
            }
        ---------------------------------------------------------------------------------------------------------------------------------------
        pthread_cond_init
            int pthread_cond_init( pthread_cond_t *cond, const pthread_condattr_t *attr );
                cond - указатель на инициализируемый объект pthread_cond_t.
                attr - NULL или указатель на объект pthread_condattr_t, определяющий атрибуты условной переменной. 

            в моем случае:
                pthread_cond_init (&q->not_full, NULL);
                pthread_cond_init (&q->not_empty, NULL);

            инициализирует условную переменную cond с указанными атрибутами attr. Если attr равно NULL, cond инициализируется 
                со значениями атрибутов по умолчанию.

            https://help.kpda.ru/neutrino/2020/help/index.jsp?topic=%2Fru.kpda.doc.os_ru%2Fhtml%2Flibraries%2Flibc%2Fp%2Fpthread_cond_init.html
            https://www.opennet.ru/man.shtml?topic=pthread_cond_init&category=3&russian=4

        pthread_cond_signal
            int pthread_cond_signal( pthread_cond_t *cond );
                cond - Указатель на объект pthread_cond_t, для которого необходимо разблокировать наиболее приоритетный поток.

            в моем случае:
                pthread_cond_signal(&q->not_empty);

            разблокирует ОДИН из заблокированных на условной переменной cond потоков, имеющий наивысший приоритет. Если 
                таких потоков несколько, pthread_cond_signal() разблокирует тот, который дольше находится в очереди ожидания.

            https://help.kpda.ru/neutrino/2020/help/index.jsp?topic=%2Fru.kpda.doc.os_ru%2Fhtml%2Flibraries%2Flibc%2Fp%2Fpthread_cond_init.html

        pthread_cond_broadcast
            int pthread_cond_broadcast( pthread_cond_t *cond );
                cond - указатель на объект pthread_cond_t, для которого необходимо разблокировать потоки.

            в моем случае:
                pthread_cond_broadcast(&q->not_full);
                pthread_cond_broadcast(&q->not_empty);

            разблокирует ВСЕ потоки, ожидающие на условной переменной cond. Сама процедура разблокирования происходит 
                с учетом приоритетов. На одном уровне приоритетов потоки разблокируются в соответствии с принципом FIFO.

            https://help.kpda.ru/neutrino/2020/help/index.jsp?topic=%2Fru.kpda.doc.os_ru%2Fhtml%2Flibraries%2Flibc%2Fp%2Fpthread_cond_init.html

        pthread_cond_destroy
            int pthread_cond_destroy( pthread_cond_t *cond );
                cond - указатель на объект pthread_cond_t.

            в моем случае:
                pthread_cond_destroy(&q->not_full);
                pthread_cond_destroy(&q->not_empty);

            уничтожает условную переменную cond. После этого ее повторное исопльзование возможно только после очередной 
                инициализации через pthread_cond_init().

            https://help.kpda.ru/neutrino/2020/help/index.jsp?topic=%2Fru.kpda.doc.os_ru%2Fhtml%2Flibraries%2Flibc%2Fp%2Fpthread_cond_init.html

        pthread_cond_wait
            int pthread_cond_wait( pthread_cond_t *cond, pthread_mutex_t *mutex );
                cond - указатель на объект pthread_cond_t, характеризующий условную переменную (condition variable) при 
                    блокировании потоков.
                mutex - мьютекс, который будет разблокирован.

            в моем случае:
                pthread_cond_wait(&q->not_full, &q->lock);

            блокирует текущий поток на условной переменной cond и разблокирует мьютекс, определяемый параметром mutex. 
                Поток, стремящийся выполнить ожидание на условной переменной, должен предварительно завладеть мьютексом 
                mutex. При возвращении управления из функции будет выполнен повторный захват мьютекса mutex текущим потоком.

            https://help.kpda.ru/neutrino/2020/help/index.jsp?topic=%2Fru.kpda.doc.os_ru%2Fhtml%2Flibraries%2Flibc%2Fp%2Fpthread_cond_init.html

    i. Оцените загрузку процессора.
        в теримнале выполняю:
            gcc queue-threads.c queue.c -o sync && ./sync
        для оценки загрузки процессора используем htop - в отдельном терминале запускаю и показываю следующее:
            работу потоков процесса sync на одном ядре (у reader() и writer() set_cpu(1);) - в htop вижу что загрузка ядра 1 100%
            работу потоков процесса sync на двух ядрах (у reader() и writer() set_cpu(1 и 2) соответственно) - загрузка ядер 1 и 2 ПОЧТИ 100%

        при mutex+cv нагрузка на одно ядро поднялась до 100%, при двух ядрах вижу что нагрузка на оба ниже чем при mutex.

        спинлок: 1: 100%; 2: 100%, 100%
        mutex: 1: 95%; 2: 95%, 95%
        mutex+cv: 1: 100%; 2: 90%, 90%

    ii. Оцените время проведенное в пользовательском режиме и в режиме
        ядра.
        в терминале пишу: 
            gcc queue-threads.c queue.c -o sync && time ./sync

        mutex+cv:
            при работе на одном ядре:
                real    0m4,604s
                user    0m3,646s
                sys     0m0,957s
            при работе на двух ядрах:
                real    0m4,410s
                user    0m3,852s
                sys     0m3,086s

        (НИЖЕ - СТАРЫЕ ДАННЫЕ ДЛЯ СРАВНЕНИЯ)
        спинлок: 
            при работе на одном ядре:
                real    0m6,457s
                user    0m6,439s
                sys     0m0,003s
            при работе на двух ядрах:
                real    0m6,445s
                user    0m12,887s
                sys     0m0,001s
        mutex:
            при работе на одном ядре:
                real    0m5,513s
                user    0m5,511s
                sys     0m0,002s
            при работе на двух ядрах:
                real    0m5,448s
                user    0m4,836s
                sys     0m4,079s
        
        из mutex: 
            вижу что системное время возросло (использование сискола futex(2) + переключения контекста), а пользовательское 
            уменьшилось, так как опять же переключение контекста (теперь мы не пребываем всегда в user space).
        ? Новое (отличие от спинлока и mutex) - изначально имею системное время не близкое к 0. Почему так?
        - даже на одном ядре pthread_cond_wait()/pthread_cond_signal() делают сисколы futex():
            pthread_cond_wait(&not_empty,&lock) - освобождает мьютекс, потом делает futex_wait() для усыпления потока в 
                ядре до получения сигнала от другого потока.
            pthread_cond_signal(&not_empty) пытается разбудить один спящий поток, через futex_wake().
            
            На двух ядрах sys время больше так как потоки работают параллельно и теперь они больше "пингуют" друг друга через 
                условную переменную.

    iii. Оцените текущую заполненность очереди, количество попыток
        чтения-записи и количество прочитанных-записанных данных.

        mutex+cd:
            при работе на одном ядре:
                queue stats: current size 46845; attempts: (10544617 10497773 46844); counts (10544617 10497772 46845)
                queue stats: current size 14833; attempts: (20989228 20974395 14833); counts (20989228 20974395 14833)
                queue stats: current size 19271; attempts: (31534700 31515430 19270); counts (31534700 31515429 19271)
            при работе на двух ядрах:
                queue stats: current size 99976; attempts: (4669230 4569253 99977); counts (4669229 4569253 99976)
                queue stats: current size 99995; attempts: (9011662 8911667 99995); counts (9011662 8911667 99995)
                queue stats: current size 99997; attempts: (13324020 13224023 99997); counts (13324020 13224023 99997)

        (НИЖЕ - СТАРЫЕ ДАННЫЕ ДЛЯ СРАВНЕНИЯ)
        спинлок: 
            при работе на одном ядре:
                queue stats: current size 100000; attempts: (17904911 17280112 624799); counts (4009080 3909080 100000)
                queue stats: current size 100000; attempts: (34557319 35505118 -947799); counts (7592519 7492519 100000)
                queue stats: current size 0; attempts: (52666334 52964686 -298352); counts (11349572 11349572 0)
            при работе на двух ядрах:
                queue stats: current size 0; attempts: (356815 10721205 -10364390); counts (356815 356815 0)
                queue stats: current size 0; attempts: (822012 21194370 -20372358); counts (822012 822012 0)
                queue stats: current size 0; attempts: (1220246 31843701 -30623455); counts (1220246 1220246 0)
            
            при работе потоков на двух ядрах вижу что add_count +- = get_count, а при работе потоков на одном ядре они не равны.
                на одном ядре writer() работает быстрее reader(), поэтому текущий размер очереди count быстро подходит к max_count, 
                поэтому writer() имеет большое количество add_attempts, но они не превращаются в реальные новые элементы в очереди.
        mutex:
            при работе на одном ядре:
                queue stats: current size 12410; attempts: (21017819 25453913 -4436094); counts (12590929 12578519 12410)
                queue stats: current size 100000; attempts: (62963065 76332570 -13369505); counts (37871033 37771033 100000)
                queue stats: current size 100000; attempts: (83874231 101820820 -17946589); counts (50419172 50319172 100000)
            при работе на двух ядрах:
                queue stats: current size 99969; attempts: (7504128 5213390 2290738); counts (5311101 5211132 99969)
                queue stats: current size 99965; attempts: (15701422 10203168 5498254); counts (10208487 10108522 99965)
                queue stats: current size 99967; attempts: (23784195 15135008 8649187); counts (15140329 15040362 99967)
            
            при зработе потоков на двух ядрах  видим, что add_count и get_count не равны при работе на двух ядрах. 
                Это связано с тем, что когда writer() в queue_add() делает pthread_mutex_unlock() (futex_wake(2)), у ядра 
                есть список потоков, ожидающих разблокировки мьютекса. ОС-й может быть выбран поток, который ранее 
                освободил мьютекс - это только что сделал writer() - его запись свежее reader()-а. 
                ИТОГО: самоотсос - writer() почти сразу получает мьютекс снова, делает queue_add(), отпускает мьютекс, 
                    забирается из списка ожидающих потоков, ...  

        Итого: не вижу разницу между mutex и mutex+cd

    Часто бывает, что поток, пишущий данные в очередь вынужден ожидать их
        (например, из сети на select()/poll()). Проэмулируйте эту ситуацию, добавив в
        поточную функцию писателя периодический вызов usleep(1). Выполните
        задания из пункта с:

        i. Оцените загрузку процессора.
            для оценки загрузки процессора используем htop - в отдельном терминале запускаю и показываю следующее:
                работу потоков процесса sync на одном ядре (у reader() и writer() set_cpu(1)) - в htop вижу что загрузка ядра 1 30%
                работу потоков процесса sync на двух ядрах (у reader() и writer() set_cpu(1 и 2) соответственно) - загрузка ядер 1 и 2 разделяет
                    30% между ядрами.
            вижу несколько причин:
                1) старая - usleep(1) дает спящие периоды у потока writer()-а.
                2) новая - условная переменная: 
                    queue пустая -> поток reader()-а засыпает пока поток writer()-а не добавит узел
                    queue не пустая -> поток writer()-а засыпает пока поток reader()-а не уберет узел

        ii. Оцените время проведенное в пользовательском режиме и в режиме
            ядра.
                в терминале пишу: 
                    gcc queue-threads.c queue.c -o sync && time ./sync

                mutex+cv+usleep(1):
                    при работе на одном ядре:
                        real    0m3,576s
                        user    0m0,194s
                        sys     0m0,537s
                    при работе на двух ядрах:
                        real    0m3,498s
                        user    0m0,246s
                        sys     0m0,607s

                (НИЖЕ - СТАРЫЕ ДАННЫЕ ДЛЯ СРАВНЕНИЯ)
                спинлок+usleep(1): 
                    при работе на одном ядре:
                        real    0m4,523s
                        user    0m4,467s
                        sys     0m0,004s
                    при работе на двух ядрах:
                        real    0m4,434s
                        user    0m4,583s
                        sys     0m0,005s
                mutex+usleep(1):
                    при работе на одном ядре:
                        real    0m5,432s
                        user    0m5,416s
                        sys     0m0,004s
                    при работе на двух ядрах:
                        real    0m5,570s
                        user    0m5,781s
                        sys     0m0,008s
                
                к user: благодаря usleep() и pthread_cond_wait() поток либо в nanosleep, либо в futex_wait(), 
                    не занимая CPU ни в user, ни в sys.
                sys - время, которое процесс провёл АКТИВНО внутри ядра (на системных вызовах: futex, nanosleep, read, write, ...).
                    то есть usleep() вызывает сискол nanosleep(), pthread_cond_wait() - futex_wait(), pthread_cond_signal() - futex_wake().
                    При этом держу в голове что writer() из‑за usleep(1) делает меньше вставок (без usleep(1) sys занимал больше).

        iii. Оцените текущую заполненность очереди, количество попыток
            чтения-записи и количество прочитанных-записанных данных.

            (НОВОЕ)
            mutex+cd+usleep(1):
                при работе на одном ядре:
                    queue stats: current size 1; attempts: (15300 15300 0); counts (15300 15299 1)
                    queue stats: current size 0; attempts: (31883 31884 -1); counts (31883 31883 0)
                    queue stats: current size 0; attempts: (48592 48593 -1); counts (48592 48592 0)    
                при работе на двух ядрах:
                    queue stats: current size 0; attempts: (16964 16965 -1); counts (16964 16964 0)
                    queue stats: current size 0; attempts: (33681 33682 -1); counts (33681 33681 0)
                    queue stats: current size 0; attempts: (50408 50409 -1); counts (50408 50408 0)
                Вижу что по сравнению с предыдущим пунктом iii. стало очень мало попыток добавления и извлечения 
                    элементов из очереди. Связываю это с тем, что поток writer()-а за время usleep(1) не делает 
                    попыток добавления элемента queue_add().
            
            (НИЖЕ - СТАРЫЕ ДАННЫЕ ДЛЯ СРАВНЕНИЯ)
            спинлок+usleep(1): 
                при работе на одном ядре:
                    queue stats: current size 0; attempts: (155 32540824 -32540669); counts (155 155 0)
                    queue stats: current size 0; attempts: (334 65426228 -65425894); counts (334 334 0)
                    queue stats: current size 0; attempts: (525 98395479 -98394954); counts (525 525 0)
                при работе на двух ядрах:
                    queue stats: current size 0; attempts: (18869 63906216 -63887347); counts (18869 18869 0)
                    queue stats: current size 0; attempts: (37755 128471974 -128434219); counts (37755 37755 0)
                    queue stats: current size 0; attempts: (56691 192986150 -192929459); counts (56691 56691 0)
                из-за usleep(1) writer() делает очень мало операций вставки элементов, по сравнению с reader(). 
                    Поэтому количество реальных операций по успешному добавлению/извлечению элементов из очереди 
                    всегла совпадает - reader() как только видит появившийся элемент в очереди, сразу его удаляет, 
                    а writer() не успевает восполнять.
            mutex+usleep(1):
                при работе на одном ядре:
                    queue stats: current size 0; attempts: (18274 66255950 -66237676); counts (18274 18274 0)
                    queue stats: current size 0; attempts: (36604 132701247 -132664643); counts (36604 36604 0)
                    queue stats: current size 0; attempts: (54886 199150221 -199095335); counts (54886 54886 0)
                при работе на двух ядрах:
                    queue stats: current size 0; attempts: (18934 69265045 -69246111); counts (18934 18934 0)
                    queue stats: current size 0; attempts: (37914 139064634 -139026720); counts (37914 37914 0)
                    queue stats: current size 0; attempts: (56905 208906817 -208849912); counts (56905 56905 0)
            


g. Используйте для синхронизации доступа к очереди семафоры. Проделайте
    задания из пунктов b, c и d. Сравните со спинлоком, мутексом и условной
    переменной.

    в терминале выполняю:
        cd ../g
        gcc queue-threads.c queue.c -o sync && ./sync
    что изменил:   


        ---------------------------------------------------------------------------------------------------------------------------------------

    i. Оцените загрузку процессора.

    ii. Оцените время проведенное в пользовательском режиме и в режиме
        ядра.

    iii. Оцените текущую заполненность очереди, количество попыток
        чтения-записи и количество прочитанных-записанных данных.

    Часто бывает, что поток, пишущий данные в очередь вынужден ожидать их
        (например, из сети на select()/poll()). Проэмулируйте эту ситуацию, добавив в
        поточную функцию писателя периодический вызов usleep(1). Выполните
        задания из пункта с:

        i. Оцените загрузку процессора.

        ii. Оцените время проведенное в пользовательском режиме и в режиме
            ядра.

        iii. Оцените текущую заполненность очереди, количество попыток
            чтения-записи и количество прочитанных-записанных данных.