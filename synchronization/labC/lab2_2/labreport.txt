2.2. Синхронизация доступа к разделяемому ресурсу
    a. Измените реализацию очереди, добавив спинлок для синхронизации доступа к
        разделяемым данным.
    b. Убедитесь, что не возникает ошибок передачи данных через очередь.
    c. Поиграйте параметрами из пункта 2.1:
        i. Оцените загрузку процессора.
        ii. Оцените время проведенное в пользовательском режиме и в режиме
            ядра.
        iii. Оцените текущую заполненность очереди, количество попыток
            чтения-записи и количество прочитанных-записанных данных.
        iv. Объясните наблюдаемые результаты.
    d. Часто бывает, что поток, пишущий данные в очередь вынужден ожидать их
        (например, из сети на select()/poll()). Проэмулируйте эту ситуацию, добавив в
        поточную функцию писателя периодический вызов usleep(1). Выполните
        задания из пункта с.
    e. Измените реализацию очереди, заменив спинлок на мутекс. Проделайте
        задания из пунктов b, c и d. Сравните со спинлоком.
    f. Измените реализацию очереди, добавив условную переменную. Проделайте
        задания из пунктов b, c и d. Сравните со спинлоком и мутексом.
    g. Используйте для синхронизации доступа к очереди семафоры. Проделайте
        задания из пунктов b, c и d. Сравните со спинлоком, мутексом и условной
        переменной.



a. Измените реализацию очереди, добавив спинлок для синхронизации доступа к
    разделяемым данным.
b. Убедитесь, что не возникает ошибок передачи данных через очередь.

    в теримнале выполняю: 
        cd a-d
        gcc queue-threads.c queue.c -o sync && ./sync
    вывод:
        бесконечный, нормальный при любом макисмальном количестве узлов процесс

    что добавил: 
        queue.h
            typedef struct {
                atomic_int lock;
            } spinlock_t;

            typedef struct _Queue {
                ...
                spinlock_t lock; 
                ...
            }

        queue.c
            static void spinlock_init(spinlock_t *s) 
                инициализирует поле lock структуры спинлока в разблокированное состояние. Я использую atomic_init lock 
                    в качестве флага, где 
                        1: лок свободен, 
                        0: лок занят. 
                    После вызова spinlock_init(), 
                    любой поток, вызвавший spinlock_lock(), увидит что mylock.lock == 1 и сможет сменить его на 0, 
                    захватив лок:
                    atomic_init(&s->lock, 1);

            static void spinlock_lock(spinlock_t *s)
                пытаемся захватить лок, пока другим потоком он не будет разблокирован. На каждой итерации сравниваем текущее значение
                    s->lock с 1: 
                        если они равны, значит лок свободен, теперь s->lock = 0 (заблокировал). Захват лока произведен успешно -> break;
                        если нет, то какой-то другой поток уже захватил лок. Повторяем попытку в новой итерации:
                while (1) {
                    int one = 1;
                    if (atomic_compare_exchange_strong(&s->lock, &one, 0)) {
                        break;
                    }
                }

            static void spinlock_unlock(spinlock_t *s)
                освобождение лока, переводя его флаг в разблокированное состояние: 
                int zero = 0;
                atomic_compare_exchange_strong(&s->lock, &zero, 1);

        ---------------------------------------------------------------------------------------------------------------------------------------
        atomic_int
            тип данных, который, в отличие от простого int, гарантирует атомарность всех операций с ним.

        ? почему используем atomic_int а не int?
        - чтобы не было гонок между потоками при проверке и установке флага lock. Если бы взяли int, то например два потока 
            могли бы прочитать лок как 1 и оба записать 0, посчитав, что они захватили лок. Но 2 потока не могут захватывать лок одновременно.
        
        atomic_init
            void atomic_init(volatile atomic_int *object, atomic_int desired)
                object - указатель на переменную типа atomic_int, которую нужно инициализировать.
                desired - значение, с которым должна быть инициализирована эта переменная.

            используется для инициализации атомарного объекта. Устанавливает начальное значение атомарной переменной.

            в моем случае: 
                atomic_init(&s->lock, 1);

        atomic_compare_exchange_strong
            bool atomic_compare_exchange_strong(volatile atomic_int *object, atomic_int *expected, atomic_int desired)
                object - указатель на переменную типа atomic_int, которую нужно изменить.
                expected - указатель на переменную типа atomic_int, содержащую ожидаемое значение.
                desired - новое значение, которое нужно установить.

            сравнивает текущее значение атомарной переменной object со значением, указанным в expected. Если они равны, то устанавливает 
                значение desired в object и возвращает trut. В противном случае, оставляет object без изменений и возвращает false.

            в моем случае:
                if (atomic_compare_exchange_strong(&s->lock, &one, 0)) {
                    break;
                }
            
            https://spec-zone.ru/cpp/atomic/atomic_compare_exchange

    НОВОЕ: добавил реализацию с использованием "официального" типа данных для спинлока от библиотеки pthread.h: 
        cd pthread_spinlock
        gcc queue-threads.c queue.c -o sync && ./sync



c. Поиграйте параметрами из пункта 2.1:
    i. Оцените загрузку процессора.
    ii. Оцените время проведенное в пользовательском режиме и в режиме
        ядра.
    iii. Оцените текущую заполненность очереди, количество попыток
        чтения-записи и количество прочитанных-записанных данных.
    iv. Объясните наблюдаемые результаты.

    i. для оценки загрузки процессора используем htop - в отдельном терминале запускаю и показываю следующее:
        работу потоков процесса sync на одном ядре (у reader() и writer() set_cpu(1);) - в htop вижу что загрузка ядра 1 100%
        работу потоков процесса sync на двух ядрах (у reader() и writer() set_cpu(1 и 2) соответственно) - загрузка ядер 1 и 2 100%

    ii. для оценки времени запускаю executable sync с time:
            gcc queue-threads.c queue.c -o sync && time ./sync
            
            при работе на одном ядре: 
                real    0m6,457s
                user    0m6,439s
                sys     0m0,003s
            при работе на двух ядрах (user = 2*real):
                real    0m6,445s
                user    0m12,887s
                sys     0m0,001s
            user - сумма времени, проведённого в user space всеми потоками, коих активных у меня 2: reader() и writer().
            sys в обоих случаях примерно нулевой так как не делаю операций в ядре (раз в секунду sleep(1) у монитора, а 
                всё остальное - чисто пользовательские операции).
        
        Альтернатива: в htop смотри цветную полосу загрузки ЦП в верхней части экрана. Цвета показывают, как распределяется процессорное время:
            green light - время в user space
            red light - время в kernel space

    iii. в терминале пишу:
            gcc queue-threads.c queue.c -o sync && ./sync

        при работе на одном ядре:
            queue stats: current size 100000; attempts: (17904911 17280112 624799); counts (4009080 3909080 100000)
            queue stats: current size 100000; attempts: (34557319 35505118 -947799); counts (7592519 7492519 100000)
            queue stats: current size 0; attempts: (52666334 52964686 -298352); counts (11349572 11349572 0)
            queue stats: current size 0; attempts: (68224857 71711105 -3486248); counts (15281929 15281929 0)
            queue stats: current size 0; attempts: (85929200 89077621 -3148421); counts (18741021 18741021 0)
            queue stats: current size 100000; attempts: (99598339 110323665 -10725326); counts (22002535 21902535 100000)
        при работе потоков на двух ядрах:
            queue stats: current size 0; attempts: (356815 10721205 -10364390); counts (356815 356815 0)
            queue stats: current size 0; attempts: (822012 21194370 -20372358); counts (822012 822012 0)
            queue stats: current size 0; attempts: (1220246 31843701 -30623455); counts (1220246 1220246 0)
            queue stats: current size 0; attempts: (1710253 42277508 -40567255); counts (1710253 1710253 0)
            queue stats: current size 0; attempts: (2084787 52976020 -50891233); counts (2084787 2084787 0)
            queue stats: current size 0; attempts: (2571947 63414633 -60842686); counts (2571947 2571947 0)
        
        При работе потоков на одном ядре add_count и get_count не равны. writer() работает быстрее reader(), поэтому текущий 
            размер очереди count быстро подходит к max_count. Значит writer() имеет большое количество add_attempts, но они 
            не превращаются в реальные новые элементы в очереди. writer() чаще освобождает лок спинлока нежели reader() -> 
            часто срабатывает это условие: 
                if (q->count == q->max_count) {
                    spinlock_unlock(&q->lock);
                    return 0;
                }
        При работе потоков на двух ядрах вижу что add_count +- = get_count. Оба потока выполняются параллельно, значит writer() 
            не имеет вышеописанную проблему как у одного ядра. reader() и writer() примерно одинаково быстро захватывают и 
            освобождают лок, что и приводит к add_count +- = get_count.

    

d. Часто бывает, что поток, пишущий данные в очередь вынужден ожидать их
    (например, из сети на select()/poll()). Проэмулируйте эту ситуацию, добавив в
    поточную функцию писателя периодический вызов usleep(1). Выполните
    задания из пункта с.

    usleep
        int usleep(useconds_t usec); 
            usec - количество микросекунд на которое нужно приостановить выполнение процесса
        
        функция приостанавливает выполнение процесса на usec микросекунд. 
        ! Остановка может продлиться несколько больше из-за системной активности или из-за того, что для осуществления вызова 
            требуется определенное время.  

    i. работа потоков процесса sync на одном ядре (у reader() и writer() set_cpu(1)) - в htop вижу что загрузка ядра 1 100%
        работу потоков процесса sync на двух ядрах (у reader() и writer() set_cpu(1 и 2) соответственно) - загрузка ТОЛЬКО ядра 1 100%

        Из-за usleep(1) поток writer() делает очень мало попыток добавления элемента в очередь - что видно в выводе:
            queue stats: current size 0; attempts: (3768 21806832 -21803064); counts (3768 3768 0)
            queue stats: current size 0; attempts: (7898 44795723 -44787825); counts (7898 7898 0)
            queue stats: current size 0; attempts: (12900 70565760 -70552860); counts (12900 12900 0)
            queue stats: current size 0; attempts: (16045 90844745 -90828700); counts (16045 16045 0)

    ii. в терминале пишу: 
            gcc queue-threads.c queue.c -o sync && time ./sync
        
        при работе на одном ядре: 
            real    0m4,523s
            user    0m4,467s
            sys     0m0,004s
        при работе на двух ядрах:
            real    0m4,434s
            user    0m4,583s
            sys     0m0,005s

        теперь user всегда незначитально превышает real, но не в 2 раза, как это было без usleep(1) на writer() потоке.
            В каждой итерации writer() спит и его ядро освобождается (видно в htop), и даже при двух зайдействованных 
            ядрах, практически всегда работает максимум один активный поток - reader().
            При этом до usleep(1) - когда writer() выполняет:
                int ok = queue_add(q, i);
            благодаря queue_add() у user время работы всегда немного больше чем у real + из определения usleep() следует 
            что оно может занимать несколько больше времени, так как для осуществления вызова требуется определенное время. 
        sys без изменений - в обоих случаях примерно нулевой в ядре делется только sleep(1) у монитора, а 
                всё остальное - операции в user space.
    
    iii. Из i.: из-за usleep(1) writer() делает очень мало операций вставки элементов, по сравнению с reader() - что видно во 
        всех выводах выше. Поэтому количество реальных операций по успешному добавлению/извлечению элементов из очереди 
        всегла совпадает - reader() как только видит появившийся элемент в очереди, сразу его удаляет, а writer() не успевает восполнять. 



    ? (d) почему usleep(0) в спинлоке на 2 ядрах влияет на CPU% в htop?
    -   1) When a thread calls sleep(0), it signals to the scheduler that it is willing to temporarily give up its current CPU time slice.
            If there are no other threads ready to run, the function returns immediately, and the thread continues execution.

            В htop вижу что состояние потока writer() практически всегда S - Sleeping (в htop отображается серым цветом). 
                Значит writer() делегирует свое время R - Running потоку reader(), который находясь в активном ожидании захватывает лок, 
                а сам writer() оставшееся маленькое время работает сам (потребляет CPU только сейчас). 
                Поток writer() будет конкурировать за CPU только когда он в состоянии R - Running.

        2)  Согласно мануалу nanosleep() (https://man7.org/linux/man-pages/man2/nanosleep.2.html): 
                nanosleep() suspends the execution of the calling thread until
                either at least the time specified in *duration has elapsed, or
                the delivery of a signal that triggers the invocation of a handler
                in the calling thread or that terminates the process.
                
                nanosleep() всегда приостанавливает выполнение потока, даже когда запрошено нулевое время -> 
                    поток переходит в состояние S - Sleeping (в htop отображается серым цветом) ->
                    планировщик это видит и выбирает другой активный поток со статусом R - Running (reader()) ->
                    reader() находясь в активном ожидании захватывает совбодный лок и работает.
        !) Согласно другому мануалу (https://pubs.opengroup.org/onlinepubs/7908799/xsh/nanosleep.html):
            The suspension time may be longer than requested because of the scheduling of other activity by the system.



    ? (e) почему нет kernel time в мьютексе с usleep() на 2 ядрах?
    -   In the default configuration, a thread tries to acquire the lock
            by initially performing an atomic test-and-set operation. If
            this operation fails, the thread goes to sleep.
                
            А так как поток writer() практически всегда имеет состояние S - Sleeping (находясь в нем мьютекс разблокирован), 
                то поток reader() может захватить мьютекс атомарно, то есть не переходя в kernel space.
            см здесь: https://alessandropellegrini.it/publications/Mar19b.pdf



e. Измените реализацию очереди, заменив спинлок на мутекс. Проделайте
    задания из пунктов b, c и d. Сравните со спинлоком.

    ? mutex vs spinlock
    -   mutex 
            когда поток пытается захватить заблокированный мьютекс:
                он блокируется и переводится в состояние ожидания (sleep)
                управление передается ядру ОС -> не тратит CPU
            когда мьютекс освобождается (какой-то второй поток который держал мьютекс его разблокирует):
                ОС пробуждает наш ожидающий поток (wake up)
                при получении кванта времени поток продолжит выполнение
            ! использует сискол futex -> требуется переключение контекста. (НО не потребляет CPU пока спит)

        spinlock
            когда поток пытается захватить заблокированный спинлок:
                поток продолжает в цикле проверять состояние лока и пытается заблокировать (значит другой поток не может занять его место)
                не освобождает CPU
            ! работает полностью в user space -> не требует переключения контекста
            ! 100% загрузка ядра CPU из-за цикла ожидания блокировки

        обобщение:
            при заблокированном мьютексе поток переводится в сон; при заблокированном локе поток постоянно пытается разблокировать.
            мьютекс использует сисколы, спинлок - нет. 
            при спящем мьютексе ОС передает ядро активному потоку, при спинлоке долбящийся поток продолжит занимать ядро.

        ответ: https://stackoverflow.com/questions/5869825/when-should-one-use-a-spinlock-instead-of-mutex

    в терминале выполняю:
        cd ../e
        gcc queue-threads.c queue.c -o sync && ./sync
    что изменил:
        queue.h
            typedef struct _Queue {
                ...
                pthread_mutex_t lock;
                ...
            }

        queue.c
            queue_t* queue_init(int max_count) {
                ...
                pthread_mutex_init(&q->lock, NULL);
                ...
            }

            void queue_destroy(queue_t *q) {
                ...
                pthread_mutex_destroy(&q->lock);
                ...
            }

        ---------------------------------------------------------------------------------------------------------------------------------------
        pthread_mutex_init
            int pthread_mutex_init(pthread_mutex_t *mutex, const pthread_mutexattr_t *attr);
                mutex - указатель на объект, который будет инициализирован как мьютекс.
                attr - указатель на объект, содержащий атрибуты мьютекса. 

            инициализирует мьютекс, используя атрибуты, указанные объектом атрибутов мьютекса attr. 
                Если attr==NULL, то мьютекс инициализируется с атрибутами по умолчанию (см. pthread_mutexattr_init()). 
                После инициализации мьютекс находится в разблокированном состоянии.

            в моем случае: 
                queue_t* queue_init(int max_count) {
                    ...
                    pthread_mutex_init(&q->lock, NULL);
                    ...
                }
            
            https://help.kpda.ru/neutrino/2020/help/index.jsp?topic=%2Fru.kpda.doc.os_ru%2Fhtml%2Flibraries%2Flibc%2Fp%2Fpthread_mutex_init.html
            https://www.opennet.ru/cgi-bin/opennet/man.cgi?topic=pthread_mutex_init
            https://pubs.opengroup.org/onlinepubs/7908799/xsh/pthread_mutex_init.html
        
        pthread_mutex_destroy
            int pthread_mutex_destroy(pthread_mutex_t *mutex);
                mutex - указатель на объект(незаблокированный мьютекс), который будет уничтожен.

            уничтожает незаблокированный мьютекс mutex. Уничтожить заблокированный мьютекс можно только 
                если вы являетесь владельцем этого мьютекса.

            в моем случае:
                void queue_destroy(queue_t *q) {
                    ...
                    pthread_mutex_destroy(&q->lock);
                    ...
                }
            
            https://help.kpda.ru/neutrino/2020/help/index.jsp?topic=%2Fru.kpda.doc.os_ru%2Fhtml%2Flibraries%2Flibc%2Fp%2Fpthread_mutex_init.html
            https://www.opennet.ru/cgi-bin/opennet/man.cgi?topic=pthread_mutex_init
            https://pubs.opengroup.org/onlinepubs/7908799/xsh/pthread_mutex_init.html
        
        pthread_mutex_lock
            int pthread_mutex_lock(pthread_mutex_t *mutex);
                mutex - указатель на объект-мьютекс, который будет заблокирован.

            блокирует мьютекс mutex. Если мьютекс уже заблокирован, то вызывающий поток блокируется до тех пор, 
                пока он не захватит мьютекс. По возвращении из функции, объект мьютекса блокируется и принадлежит 
                вызывающему потоку.

            в моем случае:
                int queue_add(queue_t *q, int val) {
                    pthread_mutex_lock(&q->lock);
                    ...
                }
            
            https://help.kpda.ru/neutrino/2020/help/index.jsp?topic=%2Fru.kpda.doc.os_ru%2Fhtml%2Flibraries%2Flibc%2Fp%2Fpthread_mutex_init.html
            https://www.opennet.ru/man.shtml?topic=pthread_mutex_lock&category=3&russian=1

        pthread_mutex_unlock
            int pthread_mutex_unlock(pthread_mutex_t *mutex);
                mutex - указатель на объект-мьютекс, который будет разблокирован.

            в моем случае: 
                int queue_add(queue_t *q, int val) {
                    ...
                    pthread_mutex_unlock(&q->lock);
                }

            разблокирует мьютекс mutex. Мьютекс должен принадлежать вызывающему потоку. Если в мьютексе есть 
                заблокированные потоки, ожидающий поток с наивысшим приоритетом разблокируется и становится 
                следующим владельцем мьютекса.

            https://help.kpda.ru/neutrino/2020/help/index.jsp?topic=%2Fru.kpda.doc.os_ru%2Fhtml%2Flibraries%2Flibc%2Fp%2Fpthread_mutex_init.html
            https://www.opennet.ru/cgi-bin/opennet/man.cgi?topic=pthread_mutex_unlock

    i. Оцените загрузку процессора.
        для оценки загрузки процессора используем htop - в отдельном терминале запускаю и показываю следующее:
            работу потоков процесса sync на одном ядре (у reader() и writer() set_cpu(1);) - в htop вижу что загрузка ядра 1 ПОЧТИ 100%
            работу потоков процесса sync на двух ядрах (у reader() и writer() set_cpu(1 и 2) соответственно) - загрузка ядер 1 и 2 ПОЧТИ 100%

        при mutex-е нагрузка на ядра стабильно незначительно ниже, чем при спинлоке.

    ii. Оцените время проведенное в пользовательском режиме и в режиме ядра.

        в терминале пишу: 
            gcc queue-threads.c queue.c -o sync && time ./sync

        (НОВОЕ)
        mutex:
            при работе на одном ядре:
                real    0m5,513s
                user    0m5,511s
                sys     0m0,002s             
            при работе на двух ядрах:
                real    0m5,448s
                user    0m4,836s
                sys     0m4,079s

        (НИЖЕ - СТАРЫЕ ДАННЫЕ ДЛЯ СРАВНЕНИЯ)
        спинлок:
            при работе на одном ядре: 
                real    0m6,457s
                user    0m6,439s
                sys     0m0,003s
            при работе на двух ядрах (user = 2*real):
                real    0m6,445s
                user    0m12,887s
                sys     0m0,001s

        при работе на одном ядре время совпадает с результатами, полученными при спинлоке.
        при работе на двух ядрах результаты отличаются от аналогичных замеров у спинлока, 
            вижу что системное время возросло (использование сискола futex(2) + переключения контекста), 
            а пользовательское уменьшилось, так как опять же переключение контекста (теперь мы не пребываем всегда в user space).
            
    iii. Оцените текущую заполненность очереди, количество попыток 
        чтения-записи и количество прочитанных-записанных данных.

        в терминале пишу:
            gcc queue-threads.c queue.c -o sync &&  ./sync

        при работе на одном ядре:
            queue stats: current size 100000; attempts: (20990816 26454828 -5464012); counts (12464902 12364902 100000)
            queue stats: current size 69992; attempts: (42316718 53364817 -11048099); counts (24501477 24431485 69992)
            queue stats: current size 45176; attempts: (62913320 79566687 -16653367); counts (37218011 37172835 45176)
            queue stats: current size 74063; attempts: (84084861 106235237 -22150376); counts (49404671 49330608 74063)
        при работе на двух ядрах:
            queue stats: current size 99978; attempts: (7154244 4570902 2583342); counts (4669411 4569433 99978)
            queue stats: current size 99990; attempts: (15019434 8350706 6668728); counts (8449227 8349237 99990)
            queue stats: current size 99994; attempts: (22789245 11973010 10816235); counts (12071535 11971541 99994)
            queue stats: current size 99959; attempts: (30663112 15952662 14710450); counts (16051152 15951193 99959)
        
        При работе на одном ядре сохраняется схожий спинлоку разброс.
        При работе на двух ядрах - нет. 
            В спинлоке при двух ядрах было так: reader() и writer() примерно одинаково быстро захватывают и 
            освобождают лок, что приводило к add_count +- = get_count. Сейчас, при захвате мьютекса видим, 
            что add_count и get_count не равны при работе на двух ядрах. 
            ? Почему?
            - когда writer() в queue_add() pthread_mutex_unlock() (futex_wake(2)), у ядра есть список потоков, 
                ожидающих разблокировки мьютекса. ОС может выбрать поток, который ранее освободил мьютекс - это 
                только что сделал writer() - его запись свежее reader()-а. 
                ИТОГО: "самоотсос" - writer() почти сразу получает мьютекс снова, делает queue_add(), отпускает мьютекс, 
                    забирается из списка ожидающих потоков, опять получает мьютекс, делает queue_add(), ...

                    

    Часто бывает, что поток, пишущий данные в очередь вынужден ожидать их
        (например, из сети на select()/poll()). Проэмулируйте эту ситуацию, добавив в
        поточную функцию писателя периодический вызов usleep(1). Выполните
        задания из пункта с:

        i. Оцените загрузку процессора.
            АНАЛОГИЧНО СПИНЛОКУ: 
                работа потоков процесса sync на одном ядре (у reader() и writer() set_cpu(1)) - в htop вижу что загрузка ядра 1 100%
                работа потоков процесса sync на двух ядрах (у reader() и writer() set_cpu(1 и 2) соответственно) - загрузка ТОЛЬКО ядра 2 100%

                Из-за usleep(1) в бесконечном цикле действие потока writer() делает очень мало попыток добавления элемента в 
                очередь - что видно в выводе (на двух ядрах):
                    queue stats: current size 0; attempts: (18654 69198083 -69179429); counts (18654 18654 0)
                    queue stats: current size 0; attempts: (37624 139127000 -139089376); counts (37624 37624 0)
                    queue stats: current size 0; attempts: (56614 209008280 -208951666); counts (56614 56614 0)
                    queue stats: current size 0; attempts: (75530 278887229 -278811699); counts (75530 75530 0)

        ii. Оцените время проведенное в пользовательском режиме и в режиме ядра.

            в терминале пишу: 
                gcc queue-threads.c queue.c -o sync && time ./sync

            при работе на одном ядре:
                real    0m5,432s
                user    0m5,416s
                sys     0m0,004s
            при работе на двух ядрах:
                real    0m5,570s
                user    0m5,781s
                sys     0m0,008s

            было на двух ядрах, без usleep(1) (см. e.c.ii. (мы сейчас в e.d.ii.)): 
                real    0m5,448s
                user    0m4,836s
                sys     0m4,079s
            Теперь user всегда незначитально превышает real, но не в 2 раза, как это было без usleep(1) на writer() потоке.
                При двух задействованных ядрах, практически всегда работает чаще всего один активный поток - reader().
                При этом до usleep(1) writer() выполняет это:
                    int ok = queue_add(q, i);
                - благодаря queue_add() у user время работы всегда немного больше чем у real + из определения usleep() следует 
                что оно может занимать несколько больше времени, так как для осуществления вызова требуется определенное время.
            
            ? куда делось sys время работы?
            - sys - время, которое процесс провёл АКТИВНО внутри ядра (на системных вызовах: futex, nanosleep, read, write, ...).
                без usleep(1) потоки "сонно" борются за lock - если один уже захватил мьютекс, то у другого попытка 
                    pthread_mutex_lock() вызывает сискол futex_wait(), а потом futex_wake() при разблокировке мьютекса.
                с usleep(1) поток writer() после каждой попытки выполнить добавление элемента в queue_add() уходит в nanosleep() 
                    (сискол вызываемый usleep()), и при этом поток уходит в состояние ожидания (sleep), не потребляя CPU.
                
                ИТОГО: sys почти занулен так как нет futex_wait()/futex_wake(), вместо них у writer()-а идут nanosleep(), 
                    а он не засчитывается в sys в sys идет только то время, которое CPU тратил на ВЫПОЛНЕНИЕ (а writer() спит) 
                    потока в kernel mode.

        iii. Оцените текущую заполненность очереди, количество попыток 
            чтения-записи и количество прочитанных-записанных данных.

            в терминале пишу: 
                gcc queue-threads.c queue.c -o sync && ./sync

            при работе на одном ядре:
                queue stats: current size 0; attempts: (17928 64360387 -64342459); counts (17928 17928 0)
                queue stats: current size 0; attempts: (36233 130787003 -130750770); counts (36233 36233 0)
                queue stats: current size 0; attempts: (54575 197254380 -197199805); counts (54575 54575 0)
                queue stats: current size 0; attempts: (72880 263781876 -263708996); counts (72880 72880 0)
            при работе на двух ядрах:
                queue stats: current size 0; attempts: (18814 68282255 -68263441); counts (18814 18814 0)
                queue stats: current size 0; attempts: (37812 138118505 -138080693); counts (37812 37812 0)
                queue stats: current size 0; attempts: (56808 208014852 -207958044); counts (56808 56808 0)
                queue stats: current size 0; attempts: (75790 277929588 -277853798); counts (75790 75790 0)

            Из i.: из-за usleep(1) writer() делает очень мало операций вставки элементов, по сравнению с reader() - что видно во 
                всех выводах выше. Поэтому количество реальных операций по успешному добавлению/извлечению элементов из очереди 
                всегла совпадает - reader() как только видит появившийся элемент в очереди, сразу его удаляет, а writer() 
                не успевает восполнять. 



f. Измените реализацию очереди, добавив условную переменную. Проделайте
    задания из пунктов b, c и d. Сравните со спинлоком и мутексом.

    ? mutex+cd vs mutex
    -   mutex: гарантия взаимного исключения критических участков кода:
                pthread_mutex_lock(&mutex);
                // критический участок кода - только один поток может работать с этой областью в единицу времени
                pthread_mutex_unlock(&mutex);
            Его проблема в том что можно сделать активное ожидание, ТРАТЯ CPU:
                while (1) {
                    if (pthread_mutex_trylock(&mutex) == 0) {
                        break;
                    }
                    // активное ожидание
                    usleep(1);
                }
            То есть вместо сна поток будет активно работать, пока не получит доступ к мьютексу.

        mutex+cd: ожидание конкретных событий в сочетании с защитой общих данных. Потоки ждут 
            наступления определенных значений у условных переменных. 
            Состоит из 2 элементов:
                1) мьютекс: защита доступа к общим данным
                2) условная переменная: механизм сигнализации о изменении состояния. 
                    Это способ засыпать внутри мьютекса только тогда, когда 
                    её состояние неподходящее, и просыпаться тогда, когда другой поток скажет, что она изменилась. 

        Итак: для ожидания условий использую связку mutex+cd.

    в терминале выполняю:
        cd ../f
        gcc queue-threads.c queue.c -o sync && ./sync
    что изменил:
        queue.h
            typedef struct _Queue {
                ...
                pthread_cond_t not_full;
                pthread_cond_t not_empty;
            }

        queue.c
            queue_t* queue_init(int max_count) {
                ...
                pthread_cond_init(&q->not_full, NULL);
                pthread_cond_init(&q->not_empty, NULL);
                ...
            }

            void queue_destroy(queue_t *q) {
                ...
                pthread_cond_broadcast(&q->not_full);
                pthread_cond_broadcast(&q->not_empty);
                ...
                pthread_cond_destroy(&q->not_full);
                pthread_cond_destroy(&q->not_empty);
                ...
            }

            int queue_add(queue_t *q, int val) {
                ...
                while (q->count == q->max_count) {
                    pthread_cond_wait(&q->not_full, &q->lock);
                }
                ...
                pthread_cond_signal(&q->not_empty);
                ...
            }

            int queue_get(queue_t *q, int *val) {
                ...
                while (q->count == 0) {
                    pthread_cond_wait(&q->not_empty, &q->lock);
                }
                ...
                pthread_cond_signal(&q->not_full);
                ...
            }
        ---------------------------------------------------------------------------------------------------------------------------------------
        pthread_cond_init
            int pthread_cond_init(pthread_cond_t *cond, const pthread_condattr_t *attr);
                *cond - указатель на инициализируемый объект.
                *attr - указатель на объект, определяющий атрибуты условной переменной. 

            инициализирует условную переменную cond с атрибутами attr. Если attr равно NULL, cond инициализируется 
                со значениями атрибутов по умолчанию.

            https://help.kpda.ru/neutrino/2020/help/index.jsp?topic=%2Fru.kpda.doc.os_ru%2Fhtml%2Flibraries%2Flibc%2Fp%2Fpthread_cond_init.html
            https://www.opennet.ru/man.shtml?topic=pthread_cond_init&category=3&russian=4

        pthread_cond_signal
            int pthread_cond_signal(pthread_cond_t *cond);
                *cond - указатель на объект, для которого необходимо разблокировать наиболее приоритетный поток.

            разблокирует ОДИН из заблокированных на условной переменной cond потоков, имеющий наивысший приоритет. Если 
                таких потоков несколько, pthread_cond_signal() разблокирует тот, который дольше находится в очереди ожидания.

            https://help.kpda.ru/neutrino/2020/help/index.jsp?topic=%2Fru.kpda.doc.os_ru%2Fhtml%2Flibraries%2Flibc%2Fp%2Fpthread_cond_init.html

        pthread_cond_broadcast
            int pthread_cond_broadcast(pthread_cond_t *cond);
                cond - указатель на объект, для которого необходимо разблокировать потоки.

            разблокирует ВСЕ потоки, ожидающие на условной переменной cond. Сама процедура разблокирования происходит 
                с учетом приоритетов. На одном уровне приоритетов потоки разблокируются в соответствии с принципом FIFO.

            https://help.kpda.ru/neutrino/2020/help/index.jsp?topic=%2Fru.kpda.doc.os_ru%2Fhtml%2Flibraries%2Flibc%2Fp%2Fpthread_cond_init.html

        pthread_cond_wait
            int pthread_cond_wait(pthread_cond_t *cond, pthread_mutex_t *mutex);
                *cond - указатель на объект - условную переменную.
                *mutex - указатель на мьютекс, который будет разблокирован.

            блокирует текущий поток на условной переменной cond и разблокирует мьютекс mutex. 
                Поток, стремящийся выполнить ожидание на условной переменной, должен предварительно завладеть мьютексом 
                mutex. При возвращении управления из функции будет выполнен повторный захват мьютекса mutex текущим потоком.

            https://help.kpda.ru/neutrino/2020/help/index.jsp?topic=%2Fru.kpda.doc.os_ru%2Fhtml%2Flibraries%2Flibc%2Fp%2Fpthread_cond_init.html

        pthread_cond_destroy
            int pthread_cond_destroy(pthread_cond_t *cond);
                cond - указатель на объект pthread_cond_t.

            уничтожает условную переменную cond. После этого ее повторное исопльзование возможно только после очередной 
                инициализации через pthread_cond_init().

            https://help.kpda.ru/neutrino/2020/help/index.jsp?topic=%2Fru.kpda.doc.os_ru%2Fhtml%2Flibraries%2Flibc%2Fp%2Fpthread_cond_init.html

    i. Оцените загрузку процессора.
        в теримнале выполняю:
            gcc queue-threads.c queue.c -o sync && ./sync
        для оценки загрузки процессора используем htop - в отдельном терминале запускаю и показываю следующее:
            работу потоков процесса sync на одном ядре (у reader() и writer() set_cpu(1);) - в htop вижу что загрузка ядра 1 100%
            работу потоков процесса sync на двух ядрах (у reader() и writer() set_cpu(1 и 2) соответственно) - загрузка ядер 1 и 2 ПОЧТИ 100%

        при mutex+cv нагрузка на одно ядро поднялась до 100%, при двух ядрах вижу что нагрузка на оба ниже чем при mutex.

        спинлок: 1: 100%; 2: 100%, 100%
        mutex: 1: 95%; 2: 95%, 95%
        mutex+cv: 1: 100%; 2: 90%, 90%

    ii. Оцените время проведенное в пользовательском режиме и в режиме
        ядра.
        в терминале пишу: 
            gcc queue-threads.c queue.c -o sync && time ./sync

        (НОВОЕ)
        mutex+cv:
            при работе на одном ядре:
                real    0m4,604s
                user    0m3,646s
                sys     0m0,957s
            при работе на двух ядрах:
                real    0m4,410s
                user    0m3,852s
                sys     0m3,086s

        (НИЖЕ - СТАРЫЕ ДАННЫЕ ДЛЯ СРАВНЕНИЯ)
        спинлок: 
            при работе на одном ядре:
                real    0m6,457s
                user    0m6,439s
                sys     0m0,003s
            при работе на двух ядрах:
                real    0m6,445s
                user    0m12,887s
                sys     0m0,001s
        mutex:
            при работе на одном ядре:
                real    0m5,513s
                user    0m5,511s
                sys     0m0,002s
            при работе на двух ядрах:
                real    0m5,448s
                user    0m4,836s
                sys     0m4,079s
        
        из mutex: 
            вижу что системное время возросло (использование сискола futex(2) + переключения контекста), а пользовательское 
            уменьшилось, так как опять же переключение контекста (теперь мы не пребываем всегда в user space).
        ? Новое (отличие от спинлока и mutex) - изначально имею sys время не близкое к 0. Почему так?
        - даже на одном ядре pthread_cond_wait()/pthread_cond_signal() делают сисколы futex():
            pthread_cond_wait(&q->not_empty, &q->lock); - освобождает мьютекс, потом делает futex_wait() для усыпления потока в 
                ядре до получения сигнала от другого потока.
            pthread_cond_signal(&not_empty) пытается разбудить один спящий поток через futex_wake().
            
            На двух ядрах sys время больше так как потоки работают параллельно и теперь они больше "пингуют" друг друга через 
                условную переменную.

    iii. Оцените текущую заполненность очереди, количество попыток
        чтения-записи и количество прочитанных-записанных данных.

        (НОВОЕ)
        mutex+cd:
            при работе на одном ядре:
                queue stats: current size 46845; attempts: (10544617 10497773 46844); counts (10544617 10497772 46845)
                queue stats: current size 14833; attempts: (20989228 20974395 14833); counts (20989228 20974395 14833)
                queue stats: current size 19271; attempts: (31534700 31515430 19270); counts (31534700 31515429 19271)
            при работе на двух ядрах:
                queue stats: current size 99976; attempts: (4669230 4569253 99977); counts (4669229 4569253 99976)
                queue stats: current size 99995; attempts: (9011662 8911667 99995); counts (9011662 8911667 99995)
                queue stats: current size 99997; attempts: (13324020 13224023 99997); counts (13324020 13224023 99997)
            
            Итого: не вижу разницу между mutex и mutex+cd (который ниже)

        (НИЖЕ - СТАРЫЕ ДАННЫЕ ДЛЯ СРАВНЕНИЯ)
        спинлок: 
            при работе на одном ядре:
                queue stats: current size 100000; attempts: (17904911 17280112 624799); counts (4009080 3909080 100000)
                queue stats: current size 100000; attempts: (34557319 35505118 -947799); counts (7592519 7492519 100000)
                queue stats: current size 0; attempts: (52666334 52964686 -298352); counts (11349572 11349572 0)
            при работе на двух ядрах:
                queue stats: current size 0; attempts: (356815 10721205 -10364390); counts (356815 356815 0)
                queue stats: current size 0; attempts: (822012 21194370 -20372358); counts (822012 822012 0)
                queue stats: current size 0; attempts: (1220246 31843701 -30623455); counts (1220246 1220246 0)
            
            при работе потоков на двух ядрах вижу что add_count +- = get_count, а при работе потоков на одном ядре они не равны.
                на одном ядре writer() работает быстрее reader(), поэтому текущий размер очереди count быстро подходит к max_count, 
                поэтому writer() имеет большое количество add_attempts, но они не превращаются в реальные новые элементы в очереди.
        mutex:
            при работе на одном ядре:
                queue stats: current size 12410; attempts: (21017819 25453913 -4436094); counts (12590929 12578519 12410)
                queue stats: current size 100000; attempts: (62963065 76332570 -13369505); counts (37871033 37771033 100000)
                queue stats: current size 100000; attempts: (83874231 101820820 -17946589); counts (50419172 50319172 100000)
            при работе на двух ядрах:
                queue stats: current size 99969; attempts: (7504128 5213390 2290738); counts (5311101 5211132 99969)
                queue stats: current size 99965; attempts: (15701422 10203168 5498254); counts (10208487 10108522 99965)
                queue stats: current size 99967; attempts: (23784195 15135008 8649187); counts (15140329 15040362 99967)
            
            при зработе потоков на двух ядрах  видим, что add_count и get_count не равны при работе на двух ядрах. 
                Это связано с тем, что когда writer() в queue_add() делает pthread_mutex_unlock() (futex_wake(2)), у ядра 
                есть список потоков, ожидающих разблокировки мьютекса. ОС-й может быть выбран поток, который ранее 
                освободил мьютекс - это только что сделал writer() - его запись свежее reader()-а. 
                ИТОГО: самоотсос - writer() почти сразу получает мьютекс снова, делает queue_add(), отпускает мьютекс, 
                    забирается из списка ожидающих потоков, ...  

    Часто бывает, что поток, пишущий данные в очередь вынужден ожидать их
        (например, из сети на select()/poll()). Проэмулируйте эту ситуацию, добавив в
        поточную функцию писателя периодический вызов usleep(1). Выполните
        задания из пункта с:

        i. Оцените загрузку процессора.
            для оценки загрузки процессора используем htop - в отдельном терминале запускаю и показываю следующее:
                работу потоков процесса sync на одном ядре (у reader() и writer() set_cpu(1)) - в htop вижу что загрузка ядра 1 30%
                работу потоков процесса sync на двух ядрах (у reader() и writer() set_cpu(1 и 2) соответственно) - загрузка ядер 1 и 2 разделяет
                    30% между ядрами.
            вижу несколько причин:
                1) старая - usleep(1) дает спящие периоды у потока writer()-а.
                2) новая - условная переменная: 
                    queue пустая -> поток reader()-а засыпает пока поток writer()-а не добавит узел
                    queue не пустая -> поток writer()-а засыпает пока поток reader()-а не уберет узел

        ii. Оцените время проведенное в пользовательском режиме и в режиме
            ядра.
                в терминале пишу: 
                    gcc queue-threads.c queue.c -o sync && time ./sync

                (НОВОЕ)
                mutex+cv+usleep(1):
                    при работе на одном ядре:
                        real    0m3,576s
                        user    0m0,194s
                        sys     0m0,537s
                    при работе на двух ядрах:
                        real    0m3,498s
                        user    0m0,246s
                        sys     0m0,607s

                (НИЖЕ - СТАРЫЕ ДАННЫЕ ДЛЯ СРАВНЕНИЯ)
                спинлок+usleep(1): 
                    при работе на одном ядре:
                        real    0m4,523s
                        user    0m4,467s
                        sys     0m0,004s
                    при работе на двух ядрах:
                        real    0m4,434s
                        user    0m4,583s
                        sys     0m0,005s
                mutex+usleep(1):
                    при работе на одном ядре:
                        real    0m5,432s
                        user    0m5,416s
                        sys     0m0,004s
                    при работе на двух ядрах:
                        real    0m5,570s
                        user    0m5,781s
                        sys     0m0,008s
                
                к user: благодаря usleep() и pthread_cond_wait() поток либо в nanosleep, либо в futex_wait(), 
                    не занимая CPU ни в user, ни в sys.
                sys - время, которое процесс провёл АКТИВНО внутри ядра (на системных вызовах: futex, nanosleep, read, write, ...).
                    то есть usleep() вызывает сискол nanosleep(), pthread_cond_wait() - futex_wait(), pthread_cond_signal() - futex_wake().
                    При этом держу в голове что writer() из‑за usleep(1) делает меньше вставок (без usleep(1) sys занимал больше).

        iii. Оцените текущую заполненность очереди, количество попыток
            чтения-записи и количество прочитанных-записанных данных.

            (НОВОЕ)
            mutex+cd+usleep(1):
                при работе на одном ядре:
                    queue stats: current size 1; attempts: (15300 15300 0); counts (15300 15299 1)
                    queue stats: current size 0; attempts: (31883 31884 -1); counts (31883 31883 0)
                    queue stats: current size 0; attempts: (48592 48593 -1); counts (48592 48592 0)    
                при работе на двух ядрах:
                    queue stats: current size 0; attempts: (16964 16965 -1); counts (16964 16964 0)
                    queue stats: current size 0; attempts: (33681 33682 -1); counts (33681 33681 0)
                    queue stats: current size 0; attempts: (50408 50409 -1); counts (50408 50408 0)
                Вижу что по сравнению с предыдущим пунктом iii. стало очень мало попыток добавления и извлечения 
                    элементов из очереди. Связываю это с тем, что поток writer()-а за время usleep(1) не делает 
                    попыток добавления элемента queue_add().
            
            (НИЖЕ - СТАРЫЕ ДАННЫЕ ДЛЯ СРАВНЕНИЯ)
            спинлок+usleep(1): 
                при работе на одном ядре:
                    queue stats: current size 0; attempts: (155 32540824 -32540669); counts (155 155 0)
                    queue stats: current size 0; attempts: (334 65426228 -65425894); counts (334 334 0)
                    queue stats: current size 0; attempts: (525 98395479 -98394954); counts (525 525 0)
                при работе на двух ядрах:
                    queue stats: current size 0; attempts: (18869 63906216 -63887347); counts (18869 18869 0)
                    queue stats: current size 0; attempts: (37755 128471974 -128434219); counts (37755 37755 0)
                    queue stats: current size 0; attempts: (56691 192986150 -192929459); counts (56691 56691 0)
                из-за usleep(1) writer() делает очень мало операций вставки элементов, по сравнению с reader(). 
                    Поэтому количество реальных операций по успешному добавлению/извлечению элементов из очереди 
                    всегла совпадает - reader() как только видит появившийся элемент в очереди, сразу его удаляет, 
                    а writer() не успевает восполнять.
            mutex+usleep(1):
                при работе на одном ядре:
                    queue stats: current size 0; attempts: (18274 66255950 -66237676); counts (18274 18274 0)
                    queue stats: current size 0; attempts: (36604 132701247 -132664643); counts (36604 36604 0)
                    queue stats: current size 0; attempts: (54886 199150221 -199095335); counts (54886 54886 0)
                при работе на двух ядрах:
                    queue stats: current size 0; attempts: (18934 69265045 -69246111); counts (18934 18934 0)
                    queue stats: current size 0; attempts: (37914 139064634 -139026720); counts (37914 37914 0)
                    queue stats: current size 0; attempts: (56905 208906817 -208849912); counts (56905 56905 0)

    ? Почему нельзя использовать 1 кондвар вместо 2?
    - 
    


g. Используйте для синхронизации доступа к очереди семафоры. Проделайте
    задания из пунктов b, c и d. Сравните со спинлоком, мутексом и условной
    переменной.

    Семафор - целочисленная переменная, общая для множества потоков. Он управляет доступом к общим ресурсам через 
        счетчик, с которым взаимодействуем через 2 функции: 
        1) sem_wait() уменьшает и если счетчик >0 то поток продолжит работать, если <= 0 то блокируется на этом семафоре.
        2) sem_post() увеличивает и если есть потоки заблокированные на этом семафоре, то один из них успешно завершит ожидание.
        ! использует системные вызовы.

    ? семафор vs мьютекс
    -   1) Семафор - целочисленная переменная, а мьютекс - объект.
        2) Объект мьютекса позволяет многим потокам обращаться к одному и тому же общему ресурсу, но в любой момент времени 
            доступ имеет только один поток. 
            Семафор позволяет нескольким потокам использовать одновременно ресурс, пока он не исчерпается (пока он >=0).
        3) С мьютексом блокировку и освобождение ресурса должен сделать один и тот же процесс. Но значение переменной семафора 
            может быть изменено любым процессом, которому нужен какой-то ресурс, но это изменение всегда атомарное, т.е. 
            в любой момент времени изменить переменную семафора может только один процесс.

    семафор vs мьютекс: https://microsin.net/programming/pc/difference-between-mutex-and-semaphore.html 

    в терминале выполняю:
        cd ../g
        gcc queue-threads.c queue.c -o sync && ./sync
    что изменил:   
        queue.h
            typedef struct _Queue {
                ...
                sem_t slots; 
                sem_t items;
            }

        queue.c
            queue_t* queue_init(int max_count) {
                ...
                // НОВОЕ (добавить можно max_count, прочитать 0)
                sem_init(&q->slots, 0, max_count);
                sem_init(&q->items, 0, 0);
                ...
            }

            void queue_destroy(queue_t *q) {
                ...
                sem_destroy(&q->slots);
                sem_destroy(&q->items);
                ...
            }

            int queue_add(queue_t *q, int val) {
                sem_wait(&q->slots); // НОВОЕ: жду, пока найдётся свободный слот
                ...
                sem_post(&q->items); // НОВОЕ: сигнал читателю что в очереди появился ещё один узел
                ...
            }

            int queue_get(queue_t *q, int *val) {
                sem_wait(&q->items); // НОВОЕ: жду пока появится хотя бы один элемент
                ...
                sem_post(&q->slots); // НОВОЕ: сигнал писателю что освободился один узел
                ...
            }
        ---------------------------------------------------------------------------------------------------------------------------------------
        sem_init
            int sem_init(sem_t *sem, int pshared, unsigned value);
                *sem - указатель на семафор sem_t, который необходимо инициализировать.
                pshared - ненулевое значение, если семафор должен быть доступен нескольким процессам через разделяемую память.
                value - начальное значение семафора. 0 - заблокированный семафор, положительное значение разблокированному. 
                    Значение не должно превышать SEM_VALUE_MAX.

            инициализирует безымянный семафор, на который ссылается аргумент sem. Начальное значение счетчика 
                семафора задается аргументом value.

            https://help.kpda.ru/neutrino/2020/help/index.jsp?topic=%2Fru.kpda.doc.os_ru%2Fhtml%2Flibraries%2Flibc%2Fs%2Fsem_init.html

        sem_wait
            int sem_wait(sem_t *sem);
                *sem - указатель на объект sem_t

            декрементирует семафор, на который ссылается sem. Если значение семафора не превышает 0, то вызывающий поток 
                блокируется, пока счетчик не увеличится или вызов не будет прерван сигналом. Штатное 
                разблокирование потока, вызвавшего sem_wait(), происходит при вызове sem_post() из другого потока.

            https://help.kpda.ru/neutrino/2020/help/index.jsp?topic=%2Fru.kpda.doc.os_ru%2Fhtml%2Flibraries%2Flibc%2Fs%2Fsem_init.html

        sem_post
            int sem_post(sem_t *sem);
                sem - указатель на объект sem_t

            инкрементирует семафор, на который ссылается аргумент sem. Если существуют потоки заблокированные на 
                этом семафоре, то один из них завершит ожидание в sem_wait(). Разблокируемый поток определяется 
                в соответствии с политиками планирования: выбирается поток с наивысшим приоритетом, который ожидал 
                дольше других потоков с таким же приоритетом.
            
            https://help.kpda.ru/neutrino/2020/help/index.jsp?topic=%2Fru.kpda.doc.os_ru%2Fhtml%2Flibraries%2Flibc%2Fs%2Fsem_init.html

        sem_destroy
            int sem_destroy(sem_t *sem);
                sem - указатель на семафор sem_t, который необходимо уничтожить.

            уничтожает безымянный семафор, на который ссылается аргумент sem. Семафор  должен быть предварительно 
                инициализирован функцией sem_init().

    i. Оцените загрузку процессора.
        в теримнале выполняю:
            gcc queue-threads.c queue.c -o sync && ./sync
        для оценки загрузки процессора используем htop - в отдельном терминале запускаю и показываю следующее:
            работу потоков процесса sync на одном ядре (у reader() и writer() set_cpu(1);) - в htop вижу что загрузка ядра 1 100%
            работу потоков процесса sync на двух ядрах (у reader() и writer() set_cpu(1 и 2) соответственно) - загрузка ядра 1 почти 100%,
                загрузка ядра 2 примерно 80%.

        спинлок: 1: 100%; 2: 100%, 100%
        mutex: 1: 95%; 2: 95%, 95%
        mutex+cv: 1: 100%; 2: 90%, 90%
        семафор: 1: 100%; 2: 90%, 80%

    ii. Оцените время проведенное в пользовательском режиме и в режиме
        ядра.
        в терминале пишу: 
            gcc queue-threads.c queue.c -o sync && time ./sync

        (НОВОЕ)
        mutex+семафор:
            при работе на одном ядре:
                real    0m5,505s
                user    0m2,230s
                sys     0m3,274s
            при работе на двух ядрах:
                real    0m5,445s
                user    0m5,296s
                sys     0m3,948s
            sys время объясняется использованием семафорами системных вызовов. А именно:
                sem_wait(&q->slots) / sem_wait(&q->items) - когда поток блокируется из‑за 
                    отсутствия элемента, вызывается futex_wait() и поток засыпает.
                sem_post(&q->items) / sem_post(&q->slots) - вызывется futex_wake(), 
                    которое тоже идёт в sys.
            ? также вижу что при работе на двух ядрах user время увеличилось. Почему?
            - на двух ядрах потоки reader() и writer() работают одновременно в user пространстве 
                (malloc()/free(), обновление счётчиков узлов очереди и тд). 

        (НИЖЕ - СТАРЫЕ ДАННЫЕ ДЛЯ СРАВНЕНИЯ)
        спинлок: 
            при работе на одном ядре:
                real    0m6,457s
                user    0m6,439s
                sys     0m0,003s
            при работе на двух ядрах:
                real    0m6,445s
                user    0m12,887s
                sys     0m0,001s
        mutex:
            при работе на одном ядре:
                real    0m5,513s
                user    0m5,511s
                sys     0m0,002s
            при работе на двух ядрах:
                real    0m5,448s
                user    0m4,836s
                sys     0m4,079s
        mutex+cv:
            при работе на одном ядре:
                real    0m4,604s
                user    0m3,646s
                sys     0m0,957s
            при работе на двух ядрах:
                real    0m4,410s
                user    0m3,852s
                sys     0m3,086s

        ОБОБЩЕНИЕ: user время вырастает почти вдвое при использовании спинлока и семафоров 
            для синхорнизации, а при использовании мьютекса, нет. Это связано с тем, что в первом случае 
            код выполняется в user space (при спинлоке полностью, при семафорах преимущественно), а
            при использовании мьютекса используется futex() сискол -> код выполняется и в kernel space.

    iii. Оцените текущую заполненность очереди, количество попыток
        чтения-записи и количество прочитанных-записанных данных.

        (НОВОЕ)
        mutex+семафоры:
            при работе на одном ядре:
                queue stats: current size 8442; attempts: (3098902 3090460 8442); counts (3098902 3090460 8442)
                queue stats: current size 1; attempts: (6132981 6132980 1); counts (6132981 6132980 1)
                queue stats: current size 1; attempts: (9193967 9193966 1); counts (9193967 9193966 1)
            при работе на двух ядрах:
                queue stats: current size 100000; attempts: (2595741 2495741 100000); counts (2595741 2495741 100000)
                queue stats: current size 99987; attempts: (4972948 4872960 99988); counts (4972947 4872960 99987)
                queue stats: current size 99915; attempts: (7430742 7330827 99915); counts (7430742 7330827 99915)

            ? При работе на двух ядрах вижу увеличение числа попыток добавления узлов в очередь. Почему?
            - пока очередь не заполнилась, writer() будет заполнять очередь так как в ней есть место. 
                reader() вынужден: 
                    1) ждать sem_wait(&q->items) когда очередь пуста.
                    2) + время на малок от writer()-а. 
                writer() быстрее чем reader(), и счетчик элементов очереди заполняется до max_count. Поэтому 
                    видем что попыток много.

        (НИЖЕ - СТАРЫЕ ДАННЫЕ ДЛЯ СРАВНЕНИЯ)
        спинлок: 
            при работе на одном ядре:
                queue stats: current size 100000; attempts: (17904911 17280112 624799); counts (4009080 3909080 100000)
                queue stats: current size 100000; attempts: (34557319 35505118 -947799); counts (7592519 7492519 100000)
                queue stats: current size 0; attempts: (52666334 52964686 -298352); counts (11349572 11349572 0)
            при работе на двух ядрах:
                queue stats: current size 0; attempts: (356815 10721205 -10364390); counts (356815 356815 0)
                queue stats: current size 0; attempts: (822012 21194370 -20372358); counts (822012 822012 0)
                queue stats: current size 0; attempts: (1220246 31843701 -30623455); counts (1220246 1220246 0)
            
            при работе потоков на двух ядрах вижу что add_count +- = get_count, а при работе потоков на одном ядре они не равны.
                на одном ядре writer() работает быстрее reader(), поэтому текущий размер очереди count быстро подходит к max_count, 
                поэтому writer() имеет большое количество add_attempts, но они не превращаются в реальные новые элементы в очереди.
        mutex:
            при работе на одном ядре:
                queue stats: current size 12410; attempts: (21017819 25453913 -4436094); counts (12590929 12578519 12410)
                queue stats: current size 100000; attempts: (62963065 76332570 -13369505); counts (37871033 37771033 100000)
                queue stats: current size 100000; attempts: (83874231 101820820 -17946589); counts (50419172 50319172 100000)
            при работе на двух ядрах:
                queue stats: current size 99969; attempts: (7504128 5213390 2290738); counts (5311101 5211132 99969)
                queue stats: current size 99965; attempts: (15701422 10203168 5498254); counts (10208487 10108522 99965)
                queue stats: current size 99967; attempts: (23784195 15135008 8649187); counts (15140329 15040362 99967)
            
            при зработе потоков на двух ядрах  видим, что add_count и get_count не равны при работе на двух ядрах. 
                Это связано с тем, что когда writer() в queue_add() делает pthread_mutex_unlock() (futex_wake(2)), у ядра 
                есть список потоков, ожидающих разблокировки мьютекса. ОС-й может быть выбран поток, который ранее 
                освободил мьютекс - это только что сделал writer() - его запись свежее reader()-а. 
                ИТОГО: самоотсос - writer() почти сразу получает мьютекс снова, делает queue_add(), отпускает мьютекс, 
                    забирается из списка ожидающих потоков, ... 
        mutex+cd:
            при работе на одном ядре:
                queue stats: current size 46845; attempts: (10544617 10497773 46844); counts (10544617 10497772 46845)
                queue stats: current size 14833; attempts: (20989228 20974395 14833); counts (20989228 20974395 14833)
                queue stats: current size 19271; attempts: (31534700 31515430 19270); counts (31534700 31515429 19271)
            при работе на двух ядрах:
                queue stats: current size 99976; attempts: (4669230 4569253 99977); counts (4669229 4569253 99976)
                queue stats: current size 99995; attempts: (9011662 8911667 99995); counts (9011662 8911667 99995)
                queue stats: current size 99997; attempts: (13324020 13224023 99997); counts (13324020 13224023 99997)

    Часто бывает, что поток, пишущий данные в очередь вынужден ожидать их
        (например, из сети на select()/poll()). Проэмулируйте эту ситуацию, добавив в
        поточную функцию писателя периодический вызов usleep(1). Выполните
        задания из пункта с:

        i. Оцените загрузку процессора.
            для оценки загрузки процессора используем htop - в отдельном терминале запускаю и показываю следующее:
                работу потоков процесса sync на одном ядре (у reader() и writer() set_cpu(1)) - в htop вижу что загрузка ядра 1 25-30%.
                работу потоков процесса sync на двух ядрах (у reader() и writer() set_cpu(1 и 2) соответственно) - загрузка ядер 1 и 2 разделяет
                    25-30% между ядрами.

            вижу несколько причин:
                1) старая - usleep(1) дает спящие периоды у потока writer()-а. 
                    (+ мониторинговый поток каждую секунду делает sleep(1)).
                2) новая - семафоры:
                    sem_wait - если семафор <=0 топоток уходит в сон в ядре, используя сискол futex_wait(2), что освобождает CPU.
                    sem_post - тоже сискол, но futex_wake(2).
            Причина скачков процентов при работе на двух ядрах - writer() быстрее reader()-а, в чем убедились в i., 
                а значит ничего не добавляет после того как добавлено max_count узлов в очередь. 

        ii. Оцените время проведенное в пользовательском режиме и в режиме
            ядра.
            в терминале пишу: 
                gcc queue-threads.c queue.c -o sync && time ./sync

            (НОВОЕ)
            mutex+семафоры:
                при работе на одном ядре:
                    real    0m4,535s
                    user    0m0,269s
                    sys     0m0,703s
                при работе на двух ядрах:
                    real    0m4,662s
                    user    0m0,270s
                    sys     0m0,901s
                ? при работе на двух ядрах sys незначительно вырос, а user не изменился. Почему?
                - оба потока параллельно выполняют свои сисколы (usleep() -> nanosleep(), sem_wait() -> futex_wait(), 
                    sem_post() -> futex_wake()), поэтому общее sys время немного больше.

            (НИЖЕ - СТАРЫЕ ДАННЫЕ ДЛЯ СРАВНЕНИЯ)
            спинлок+usleep(1): 
                при работе на одном ядре:
                    real    0m4,523s
                    user    0m4,467s
                    sys     0m0,004s
                при работе на двух ядрах:
                    real    0m4,434s
                    user    0m4,583s
                    sys     0m0,005s
            mutex+usleep(1):
                при работе на одном ядре:
                    real    0m5,432s
                    user    0m5,416s
                    sys     0m0,004s
                при работе на двух ядрах:
                    real    0m5,570s
                    user    0m5,781s
                    sys     0m0,008s
            mutex+cv+usleep(1):
                при работе на одном ядре:
                    real    0m3,576s
                    user    0m0,194s
                    sys     0m0,537s
                при работе на двух ядрах:
                    real    0m3,498s
                    user    0m0,246s
                    sys     0m0,607s

        iii. Оцените текущую заполненность очереди, количество попыток
            чтения-записи и количество прочитанных-записанных данных.

            (НОВОЕ)
            mutex+семафоры:
                при работе на одном ядре:
                    queue stats: current size 0; attempts: (14805 14805 0); counts (14805 14805 0)
                    queue stats: current size 0; attempts: (30142 30142 0); counts (30142 30142 0)
                    queue stats: current size 0; attempts: (46946 46946 0); counts (46946 46946 0)
                при работе на двух ядрах:
                    queue stats: current size 0; attempts: (16628 16628 0); counts (16628 16628 0)
                    queue stats: current size 0; attempts: (33271 33271 0); counts (33271 33271 0)
                    queue stats: current size 0; attempts: (49996 49996 0); counts (49996 49996 0)
                каждое появление элемента сразу же считывается - очередь не накапливает элементы.
            
            (НИЖЕ - СТАРЫЕ ДАННЫЕ ДЛЯ СРАВНЕНИЯ)
            спинлок+usleep(1): 
                при работе на одном ядре:
                    queue stats: current size 0; attempts: (155 32540824 -32540669); counts (155 155 0)
                    queue stats: current size 0; attempts: (334 65426228 -65425894); counts (334 334 0)
                    queue stats: current size 0; attempts: (525 98395479 -98394954); counts (525 525 0)
                при работе на двух ядрах:
                    queue stats: current size 0; attempts: (18869 63906216 -63887347); counts (18869 18869 0)
                    queue stats: current size 0; attempts: (37755 128471974 -128434219); counts (37755 37755 0)
                    queue stats: current size 0; attempts: (56691 192986150 -192929459); counts (56691 56691 0)
                из-за usleep(1) writer() делает очень мало операций вставки элементов, по сравнению с reader(). 
                    Поэтому количество реальных операций по успешному добавлению/извлечению элементов из очереди 
                    всегла совпадает - reader() как только видит появившийся элемент в очереди, сразу его удаляет, 
                    а writer() не успевает восполнять.
            mutex+usleep(1):
                при работе на одном ядре:
                    queue stats: current size 0; attempts: (18274 66255950 -66237676); counts (18274 18274 0)
                    queue stats: current size 0; attempts: (36604 132701247 -132664643); counts (36604 36604 0)
                    queue stats: current size 0; attempts: (54886 199150221 -199095335); counts (54886 54886 0)
                при работе на двух ядрах:
                    queue stats: current size 0; attempts: (18934 69265045 -69246111); counts (18934 18934 0)
                    queue stats: current size 0; attempts: (37914 139064634 -139026720); counts (37914 37914 0)
                    queue stats: current size 0; attempts: (56905 208906817 -208849912); counts (56905 56905 0)
            mutex+cd+usleep(1):
                при работе на одном ядре:
                    queue stats: current size 1; attempts: (15300 15300 0); counts (15300 15299 1)
                    queue stats: current size 0; attempts: (31883 31884 -1); counts (31883 31883 0)
                    queue stats: current size 0; attempts: (48592 48593 -1); counts (48592 48592 0)    
                при работе на двух ядрах:
                    queue stats: current size 0; attempts: (16964 16965 -1); counts (16964 16964 0)
                    queue stats: current size 0; attempts: (33681 33682 -1); counts (33681 33681 0)
                    queue stats: current size 0; attempts: (50408 50409 -1); counts (50408 50408 0)